{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "Changed value of parameter TimeLimit to 100.0\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n"
     ]
    }
   ],
   "source": [
    "import gurobipy as grb\n",
    "import numpy as np\n",
    "\n",
    "opt_model = grb.Model(name=\"MIP Model\")\n",
    "opt_model.setParam(\"NonConvex\", 2)\n",
    "opt_model.setParam(\"TimeLimit\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens dataset from /home/nick/datasets/gurobi_data/ with mode sparse..\n",
      "Column names are userId, movieId, rating, timestamp\n",
      "Processed 1140 lines. 10 users x 867 movies.\n",
      "Dataset contains 1139 ratings (13.137254901960786% matrix density)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "from dataset import MovieLensDataset\n",
    "d = MovieLensDataset('/home/nick/datasets/gurobi_data/', mode='sparse')\n",
    "# d = MovieLensDataset('data/', mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = d.X.nonzero()\n",
    "ratings = d.X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = opt_model.addVars(d.n_users, lb=-20.0, ub=20.0, vtype=grb.GRB.CONTINUOUS, name=\"users\")\n",
    "v = opt_model.addVars(d.n_movies,lb=-20.0, ub=20.0, vtype=grb.GRB.CONTINUOUS, name=\"movies\")\n",
    "# theta = opt_model.addVars(d.n_users, d.n_movies, vtype=grb.GRB.INTEGER)\n",
    "# x_vars  ={(i,j):opt_model.addVar(vtype=grb.GRB.INTEGER,\n",
    "#                         name=\"x_{0}_{1}\".format(i,j)) for i in rows for j in cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective = grb.quicksum((u[i]*v[j] - d.X[i,j])**2\n",
    "#                          for i, j in zip(rows, cols))\n",
    "# objective = sum((u[rows[i]]*v[cols[i]] - d.X[rows[i], cols[i]]) for i in range(len(ratings)))\n",
    "objective = 0\n",
    "for i, j in zip(rows, cols):\n",
    "    z = opt_model.addVar(name=f'z_{i}_{j}')\n",
    "    opt_model.addConstr(u[i]*v[j] == z)\n",
    "    objective += (z - d.X[i, j]) * (z - d.X[i, j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.1.1 build v9.1.1rc0 (linux64)\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "Optimize a model with 0 rows, 2016 columns and 0 nonzeros\n",
      "Model fingerprint: 0xf6b71ef4\n",
      "Model has 1139 quadratic objective terms\n",
      "Model has 1139 quadratic constraints\n",
      "Coefficient statistics:\n",
      "  Matrix range     [0e+00, 0e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  QLMatrix range   [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 2e+01]\n",
      "  QObjective range [2e+00, 2e+00]\n",
      "  Bounds range     [2e+01, 2e+01]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "\n",
      "Continuous model is non-convex -- solving as a MIP.\n",
      "\n",
      "Presolve time: 0.00s\n",
      "Presolved: 4556 rows, 2016 columns, 13668 nonzeros\n",
      "Presolved model has 1139 quadratic objective terms\n",
      "Presolved model has 1139 bilinear constraint(s)\n",
      "Variable types: 2016 continuous, 0 integer (0 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 1701 iterations, 0.05 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0 1075          -    0.00000      -     -    0s\n",
      "H    0     0                    22028.530044    0.00000   100%     -    0s\n",
      "H    0     0                    21945.134456    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0 1138 21945.1345    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0 1138 21945.1345    0.00000   100%     -    0s\n",
      "H    0     0                    21271.728026    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0 1138 21271.7280    0.00000   100%     -    0s\n",
      "H    0     0                    14400.087275    0.00000   100%     -    0s\n",
      "H    0     0                    13640.419100    0.00000   100%     -    0s\n",
      "H    0     0                    12196.816400    0.00000   100%     -    0s\n",
      "H    0     0                    1421.4868305    0.00000   100%     -    0s\n",
      "     0     2    0.00000    0 1138 1421.48683    0.00000   100%     -    0s\n",
      "  2885  1429  679.43891   68 1139 1421.48683    0.00000   100%  52.5    5s\n",
      "H 9290  5522                    1315.7002748    0.00000   100%  29.8    9s\n",
      "  9399  5806 1194.57641  348 1139 1315.70027    0.00000   100%  29.6   10s\n",
      "H14497  7873                    1234.4484763    0.00000   100%  23.0   15s\n",
      " 21337 13477 1060.42182  149 1139 1234.44848    0.16067   100%  20.9   20s\n",
      " 22275 13499 1141.19536  171 1139 1234.44848    0.16067   100%  20.7   27s\n",
      " 26403 17965 1046.14818   40 1138 1234.44848   78.91365  93.6%  19.7   30s\n",
      " 32815 22750     cutoff   55      1234.44848  136.56670  88.9%  19.1   35s\n",
      "H36253 24304                    1233.5787614  146.76263  88.1%  19.1   37s\n",
      " 39624 28056  645.62120   62 1139 1233.57876  154.27853  87.5%  18.9   40s\n",
      " 47194 33762  341.02843   36 1138 1233.57876  173.25254  86.0%  18.1   46s\n",
      " 52954 38647 1061.97544  135 1139 1233.57876  173.25254  86.0%  17.5   50s\n",
      "H59230 42135                    1232.9337949  184.57000  85.0%  16.9   53s\n",
      " 61234 44246  759.71230   69 1130 1232.93379  184.57000  85.0%  16.8   55s\n",
      "H67499 47970                    1232.4050818  184.57000  85.0%  16.4   58s\n",
      "H69551 48635                    1232.0765724  184.57000  85.0%  16.4   59s\n",
      " 69619 49599 1166.20343  109 1130 1232.07657  184.57000  85.0%  16.4   61s\n",
      " 76213 54327 1225.07516  156 1130 1232.07657  184.57000  85.0%  16.3   65s\n",
      " 84131 59908 1227.36758  220 1107 1232.07657  184.57000  85.0%  16.1   70s\n",
      " 89686 64520 1227.40602  317 1026 1232.07657  184.57000  85.0%  16.3   75s\n",
      " 97580 70890 1227.41550  380  987 1232.07657  184.57000  85.0%  15.9   80s\n",
      " 105786 76723 1227.46237  467  927 1232.07657  184.57000  85.0%  15.8   85s\n",
      " 112314 81606 1231.41378  562  888 1232.07657  184.57000  85.0%  15.9   90s\n",
      " 119168 87237 1229.22027  669  861 1232.07657  184.57000  85.0%  15.8   95s\n",
      " 126084 91798 1227.52833  835  844 1232.07657  184.57000  85.0%  16.0  100s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 236\n",
      "\n",
      "Explored 127022 nodes (2034601 simplex iterations) in 100.03 seconds\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 10: 1232.08 1232.41 1232.93 ... 14400.1\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.232076572445e+03, best bound 1.845700034225e+02, gap 85.0196%\n"
     ]
    }
   ],
   "source": [
    "opt_model.setObjective(objective, grb.GRB.MINIMIZE)\n",
    "opt_model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44687629, -0.46980928, -0.41985584, -0.41530558, -0.4244836 ,\n",
       "       -0.3       , -0.5       , -0.42671747, -0.47669432, -0.5       ,\n",
       "       -0.42700774, -0.47486636, -0.35005477, -0.4257459 , -0.33028434,\n",
       "       -0.5485825 , -0.31501264, -0.357699  , -0.49214009, -0.43068868,\n",
       "       -0.4594099 , -0.42491291, -0.4060017 , -0.30111534, -0.28812664,\n",
       "       -0.48069   , -0.47868411, -0.34767677, -0.45877194, -0.37141814,\n",
       "       -0.37141814, -0.44942539, -0.50584471, -0.34731257, -0.4783556 ,\n",
       "       -0.46831755, -0.44340898, -0.36534367, -0.50185889, -0.30111534,\n",
       "       -0.4       , -0.41526294, -0.44208543, -0.40148712, -0.54408255,\n",
       "       -0.5       , -0.5       , -0.40148712, -0.50185889, -0.30111534,\n",
       "       -0.5       , -0.50185889, -0.49430412, -0.50185889, -0.30111534,\n",
       "       -0.50185889, -0.50185889, -0.37141814, -0.52642953, -0.33716228,\n",
       "       -0.44346953, -0.54408255, -0.50185889, -0.4       , -0.50185889,\n",
       "       -0.5       , -0.40148712, -0.54408255, -0.51253246, -0.54408255,\n",
       "       -0.46618363, -0.5       , -0.42071725, -0.4762851 , -0.49430412,\n",
       "       -0.40148712, -0.3943032 , -0.50103862, -0.50185889, -0.50185889,\n",
       "       -0.50185889, -0.52589657, -0.50185889, -0.30111534, -0.43526604,\n",
       "       -0.51763575, -0.54161425, -0.50185889, -0.54408255, -0.49500629,\n",
       "       -0.50185889, -0.4       , -0.30111534, -0.30111534, -0.30111534,\n",
       "       -0.30111534, -0.40148712, -0.43526604, -0.46123316, -0.4       ,\n",
       "       -0.50185889, -0.32644953, -0.5811457 , -0.37687895, -0.40148712,\n",
       "       -0.5       , -0.30111534, -0.30111534, -0.49500629, -0.40148712,\n",
       "       -0.40148712, -0.5       , -0.40148712, -0.40148712, -0.5       ,\n",
       "       -0.50103862, -0.48432049, -0.40148712, -0.50185889, -0.4       ,\n",
       "       -0.42302937, -0.40148712, -0.50185889, -0.40148712, -0.5       ,\n",
       "       -0.40148712, -0.5       , -0.54408255, -0.42302937, -0.3       ,\n",
       "       -0.50185889, -0.40148712, -0.4       , -0.40282788, -0.5       ,\n",
       "       -0.5       , -0.50185889, -0.5       , -0.50185889, -0.40148712,\n",
       "       -0.50185889, -0.48432049, -0.40148712, -0.20074356, -0.4       ,\n",
       "       -0.40148712, -0.5       , -0.50185889, -0.20074356, -0.4514368 ,\n",
       "       -0.40148712, -0.50185889, -0.2       , -0.44597066, -0.43526604,\n",
       "       -0.30111534, -0.50185889, -0.40148712, -0.50185889, -0.50185889,\n",
       "       -0.40148712, -0.40148712, -0.50185889, -0.30111534, -0.52589657,\n",
       "       -0.50185889, -0.29060566, -0.50185889, -0.50185889, -0.40148712,\n",
       "       -0.20074356, -0.2490501 , -0.37108876, -0.50185889, -0.4       ,\n",
       "       -0.40148712, -0.50185889, -0.3       , -0.54408255, -0.50185889,\n",
       "       -0.50185889, -0.50185889, -0.40148712, -0.40148712, -0.45728216,\n",
       "       -0.50185889, -0.50185889, -0.40148712, -0.5       , -0.50185889,\n",
       "       -0.5       , -0.50185889, -0.32934688, -0.40148712, -0.50185889,\n",
       "       -0.50185889, -0.5       , -0.43526604, -0.49430412, -0.5       ,\n",
       "       -0.50185889, -0.50185889, -0.40148712, -0.50185889, -0.40148712,\n",
       "       -0.25597986, -0.3       , -0.30111534, -0.50185889, -0.5       ,\n",
       "       -0.49500629, -0.40148712, -0.4       , -0.50185889, -0.50185889,\n",
       "       -0.50185889, -0.40148712, -0.28812664, -0.40148712, -0.40641414,\n",
       "       -0.40148712, -0.40148712, -0.50185889, -0.50185889, -0.60091142,\n",
       "       -0.50185889, -0.4       , -0.4       , -0.4514368 , -0.38619791,\n",
       "       -0.40148712, -0.50185889, -0.43530041, -0.32178375, -0.42082006,\n",
       "       -0.43274506, -0.4225091 , -0.27760533, -0.47532274, -0.52813638,\n",
       "       -0.47342256, -0.31688183, -0.42082006, -0.31561504, -0.4225091 ,\n",
       "       -0.47342256, -0.52813638, -0.4225091 , -0.52813638, -0.36969547,\n",
       "       -0.26301253, -0.36969547, -0.52602507, -0.31561504, -0.4225091 ,\n",
       "       -0.21125455, -0.36969547, -0.52813638, -0.52813638, -0.28168972,\n",
       "       -0.36507028, -0.10209415, -0.1020364 , -1.02094149, -0.47610178,\n",
       "       -0.10209415, -0.10209415, -0.10209415, -0.10209415, -0.10209415,\n",
       "       -0.61256489, -0.10209415, -0.81675319, -0.10209415, -1.02094149,\n",
       "       -0.91884734, -0.10209415, -0.10209415, -1.02094149, -0.10209415,\n",
       "       -1.02094149, -1.02094149, -0.91884734, -1.02094149, -0.10209415,\n",
       "       -1.02094149, -0.91884734, -1.02094149, -0.91884734, -1.02094149,\n",
       "       -0.10209415, -0.36664033, -0.34039537, -0.34568299, -0.36251568,\n",
       "       -0.42345882, -0.48335424, -0.60419281, -0.23789773, -0.60419281,\n",
       "       -0.34933659, -0.60930689, -0.24167712, -0.60419281, -0.29862182,\n",
       "       -0.5243956 , -0.4718998 , -0.418336  , -0.60419281, -0.45577827,\n",
       "       -0.48744551, -0.40073063, -0.40586301, -0.36919725, -0.52652228,\n",
       "       -0.24167712, -0.35081537, -0.5357687 , -0.60419281, -0.23789773,\n",
       "       -0.60419281, -0.26746911, -0.47468479, -0.47349389, -0.24372276,\n",
       "       -0.46578213, -0.36251568, -0.48744551, -0.48335424, -0.60930689,\n",
       "       -0.48744551, -0.48335424, -0.48335424, -0.60419281, -0.60419281,\n",
       "       -0.60037552, -0.4618468 , -0.60419281, -0.36251568, -0.60419281,\n",
       "       -0.36251568, -0.60930689, -0.24372276, -0.46080986, -0.60930689,\n",
       "       -0.36251568, -0.60419281, -0.48335424, -0.14661434, -0.60419281,\n",
       "       -0.24372276, -0.60419281, -0.12083856, -0.48744551, -0.60419281,\n",
       "       -0.48744551, -0.48335424, -0.24167712, -0.60419281, -0.48744551,\n",
       "       -0.36251568, -0.12083856, -0.60930689, -0.48744551, -0.12083856,\n",
       "       -0.48744551, -0.48335424, -0.60930689, -0.36251568, -0.60930689,\n",
       "       -0.24167712, -0.48335424, -0.36251568, -0.24167712, -0.48335424,\n",
       "       -0.45744843, -0.48335424, -0.12186138, -0.25902642, -0.60419281,\n",
       "       -0.24167712, -0.48335424, -0.41377308, -0.48335424, -0.48335424,\n",
       "       -0.36251568, -0.60930689, -0.60930689, -0.48335424, -0.60930689,\n",
       "       -0.12186138, -0.17700127, -0.24372276, -0.48335424, -0.48335424,\n",
       "       -0.60419281, -0.48335424, -0.60419281, -0.60419281, -0.34536856,\n",
       "       -0.24167712, -0.39814699, -0.12083856, -0.24167712, -0.60419281,\n",
       "       -0.60419281, -0.48744551, -0.48335424, -0.60930689, -0.24372276,\n",
       "       -0.60419281, -0.48335424, -0.36251568, -0.60419281, -0.48744551,\n",
       "       -0.12083856, -0.48335424, -0.48744551, -0.48335424, -0.36558413,\n",
       "       -0.60419281, -0.49551555, -0.24372276, -0.60419281, -0.24167712,\n",
       "       -0.12186138, -0.36251568, -0.60419281, -0.48744551, -0.36251568,\n",
       "       -0.48335424, -0.48744551, -0.36558413, -0.12083856, -0.36558413,\n",
       "       -0.48744551, -0.36558413, -0.36558413, -0.48335424, -0.60419281,\n",
       "       -0.36251568, -0.48335424, -0.36251568, -0.36251568, -0.24167712,\n",
       "       -0.60930689, -0.38648974, -0.36251568, -0.24372276, -0.48744551,\n",
       "       -0.12083856, -0.48744551, -0.36251568, -0.12083856, -0.36251568,\n",
       "       -0.60930689, -0.36251568, -0.46530385, -0.48335424, -0.60419281,\n",
       "       -0.47733227, -0.50686732, -0.33559547, -0.42821225, -0.29155408,\n",
       "       -0.40576861, -0.4486762 , -0.22524735, -0.57221529, -0.34332917,\n",
       "       -0.33726026, -0.48473356, -0.38875062, -0.33726026, -0.4486762 ,\n",
       "       -0.39444723, -0.39244665, -0.34332917, -0.31727396, -0.5046157 ,\n",
       "       -0.3898166 , -0.43465265, -0.33159897, -0.55931193, -0.44744954,\n",
       "       -0.33159897, -0.27477838, -0.43465265, -0.33558716, -0.44744954,\n",
       "       -0.44213196, -0.44744954, -0.22106598, -0.55266495, -0.44213196,\n",
       "       -0.33159897, -0.44213196, -0.33159897, -0.36673132, -0.44213196,\n",
       "       -0.44744954, -0.44213196, -0.44213196, -0.44213196, -0.44213196,\n",
       "       -0.33159897, -0.33558716, -0.44213196, -0.33558716, -0.55931193,\n",
       "       -0.33159897, -0.22106598, -0.44213196, -0.44744954, -0.44744954,\n",
       "       -0.44744954, -0.33558716, -0.11186239, -0.44744954, -0.33159897,\n",
       "       -0.44213196, -0.33159897, -0.33558716, -0.33159897, -0.38479254,\n",
       "       -0.44213196, -0.44213196, -0.44744954, -0.33558716, -0.55266495,\n",
       "       -0.33159897, -0.39114664, -0.55266495, -0.33159897, -0.33159897,\n",
       "       -0.33558716, -0.11053299, -0.22106598, -0.33159897, -0.43671384,\n",
       "       -0.38530655, -0.33159897, -0.22106598, -0.44213196, -0.33558716,\n",
       "       -0.33159897, -0.33159897, -0.33159897, -0.44213196, -0.33004247,\n",
       "       -0.33159897, -0.44744954, -0.44213196, -0.33159897, -0.22106598,\n",
       "       -0.33159897, -0.33558716, -0.44213196, -0.55266495, -0.22372477,\n",
       "       -0.38479254, -0.44213196, -0.33558716, -0.33558716, -0.33159897,\n",
       "       -0.55266495, -0.44213196, -0.33558716, -0.38479254, -0.33558716,\n",
       "       -0.33159897, -0.55266495, -0.33159897, -0.55266495, -0.33159897,\n",
       "       -0.33159897, -0.44744954, -0.33159897, -0.44744954, -0.44213196,\n",
       "       -0.55266495, -0.33159897, -0.44744954, -0.32598949, -0.22372477,\n",
       "       -0.33159897, -0.33159897, -0.37880813, -0.33558716, -0.33159897,\n",
       "       -0.33159897, -0.44213196, -0.33558716, -0.11053299, -0.33159897,\n",
       "       -0.55266495, -0.44744954, -0.37880813, -0.11053299, -0.44744954,\n",
       "       -0.33558716, -0.33159897, -0.33159897, -0.55266495, -0.37880813,\n",
       "       -0.44213196, -0.33159897, -0.33159897, -0.55266495, -0.33558716,\n",
       "       -0.44213196, -0.44744954, -0.44744954, -0.33558716, -0.33558716,\n",
       "       -0.33159897, -0.44213196, -0.33558716, -0.44744954, -0.3167582 ,\n",
       "       -0.33159897, -0.49480669, -0.33159897, -0.33159897, -0.33558716,\n",
       "       -0.33558716, -0.33558716, -0.33159897, -0.44213196, -0.33558716,\n",
       "       -0.33159897, -0.33558716, -0.32901445, -0.44213196, -0.33159897,\n",
       "       -0.33159897, -0.38479254, -0.33159897, -0.43465265, -0.33159897,\n",
       "       -0.33159897, -0.33159897, -0.38894647, -0.44213196, -0.44744954,\n",
       "       -0.33558716, -0.44744954, -0.33159897, -0.33558716, -0.33159897,\n",
       "       -0.44213196, -0.33558716, -0.33558716, -0.33159897, -0.33159897,\n",
       "       -0.33159897, -0.44744954, -0.33558716, -0.44213196, -0.33159897,\n",
       "       -0.33159897, -0.33159897, -0.33159897, -0.33159897, -0.33159897,\n",
       "       -0.44213196, -0.22106598, -0.33159897, -0.44744954, -0.33159897,\n",
       "       -0.36467636, -0.33159897, -0.44213196, -0.55931193, -0.33159897,\n",
       "       -0.44744954, -0.44744954, -0.11053299, -0.33159897, -0.33159897,\n",
       "       -0.33159897, -0.33558716, -0.44213196, -0.33159897, -0.55266495,\n",
       "       -0.33159897, -0.44744954, -0.22106598, -0.33159897, -0.11186239,\n",
       "       -0.33159897, -0.33159897, -0.55266495, -0.33558716, -0.33159897,\n",
       "       -0.33159897, -0.44213196, -0.33558716, -0.55266495, -0.44744954,\n",
       "       -0.33159897, -0.44213196, -0.33159897, -0.33159897, -0.44744954,\n",
       "       -0.44213196, -0.33159897, -0.33159897, -0.33558716, -0.33558716,\n",
       "       -0.33558716, -0.33558716, -0.33159897, -0.44744954, -0.44744954,\n",
       "       -0.44213196, -0.22372477, -0.44213196, -0.44213196, -0.44213196,\n",
       "       -0.44213196, -0.44213196, -0.45074559, -0.45074559, -0.45074559,\n",
       "       -0.1690296 , -0.56343199, -0.45074559, -0.33805919, -0.22345639,\n",
       "       -0.45074559, -0.50855432, -0.27897823, -0.16738694, -0.39440239,\n",
       "       -0.50708879, -0.1690296 , -0.1126864 , -0.56343199, -0.44636517,\n",
       "       -0.28171599, -0.33805919, -0.45074559, -0.22318258, -0.53036848,\n",
       "       -0.0563432 , -0.50216081, -0.1126864 , -0.16738694, -0.1690296 ,\n",
       "       -0.33805919, -0.45074559, -0.45074559, -0.50216081, -0.50216081,\n",
       "       -0.39593707, -0.39440239, -0.19265381, -0.42235664, -0.33477388,\n",
       "       -0.33477388, -0.1690296 , -0.56343199, -0.50216081, -0.50216081,\n",
       "       -0.44636517, -0.50216081, -0.44636517, -0.45074559, -0.0563432 ,\n",
       "       -0.50708879, -0.50708879, -0.2253728 , -0.50708879, -0.45074559,\n",
       "       -0.33805919, -0.45074559, -0.50708879, -0.39056952, -0.28171599,\n",
       "       -0.2253728 , -0.50216081, -0.50708879, -0.39440239, -0.1126864 ,\n",
       "       -0.1126864 , -0.16738694, -0.11159129, -0.39056952, -0.1126864 ,\n",
       "       -0.50708879, -0.45074559, -0.45074559, -0.39440239, -0.45074559,\n",
       "       -0.1126864 , -0.27897823, -0.27897823, -0.45074559, -0.33805919,\n",
       "       -0.39440239, -0.22318258, -0.39440239, -0.2253728 , -0.16738694,\n",
       "       -0.22318258, -0.39440239, -0.39440239, -0.1126864 , -0.2253728 ,\n",
       "       -0.1126864 , -0.33805919, -0.50216081, -0.39056952, -0.39440239,\n",
       "       -0.50708879, -0.45074559, -0.50708879, -0.1126864 , -0.0563432 ,\n",
       "       -0.1126864 , -0.50708879, -0.28171599, -0.50216081, -0.39440239,\n",
       "       -0.05579565, -0.33805919, -0.32851478, -0.3       , -0.3       ,\n",
       "       -0.4       , -0.2       , -0.40000001, -0.4       , -0.2       ,\n",
       "       -0.3       , -0.3       , -0.50000001, -0.3       , -0.2       ,\n",
       "       -0.2       , -0.3       , -0.3       , -0.1       , -0.1       ,\n",
       "       -0.40000001, -0.40000001, -0.50000001, -0.1       , -0.1       ,\n",
       "       -0.3       , -0.3       , -0.4       , -0.1       , -0.40000001,\n",
       "       -0.50000001, -0.40000001, -0.1       , -0.4       , -0.2       ,\n",
       "       -0.4       , -0.2       , -0.07432487, -0.44498736, -0.44498736,\n",
       "       -0.44594919, -0.44594919])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_arr = np.array([u[i].X for i in range(d.n_users)])\n",
    "v_arr = np.array([v[i].X for i in range(d.n_movies)])\n",
    "v_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_arr.shape\n",
    "u_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1) (867, 1) (10, 867) sparse\n",
      "Test set MSE: 1.0817177984594597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0817177984594597"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_arr = u_arr.reshape(-1, 1)\n",
    "v_arr = v_arr.reshape(-1, 1)\n",
    "\n",
    "# let's now evaluate the MEAN squared error (above we're just dealing with sum of squared errors)\n",
    "from main import evaluate\n",
    "mse = evaluate(u_arr, v_arr, d.X, 'sparse')\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221.513500325544"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see whether fun eval matches\n",
    "from als import ALSSparse\n",
    "als = ALSSparse(u=u_arr, v=v_arr, dataset=d.X)\n",
    "als.function_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12274293.05495823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check saved vectors\n",
    "u = np.load('./data/gurobi_U.npy')\n",
    "u = u.astype(np.float64)\n",
    "v = np.load('./data/gurobi_V.npy')\n",
    "v = v.astype(np.float64)\n",
    "from als import ALSSparse, ALS\n",
    "als = ALS(u=u, v=v, dataset=d.X.A)\n",
    "als.function_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (867,) (10, 867) full\n",
      "Test set MSE: 10776.37669443216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10776.37669443216"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import evaluate\n",
    "mse = evaluate(u, v, d.X, 'full')\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "\n",
    "def f(theta, nusers):\n",
    "    u = theta[:nusers].reshape(-1, 1)\n",
    "    v = theta[nusers:].reshape(-1, 1)\n",
    "    return np.sum( (M*(u@v.T)-X)**2 )\n",
    "X = d.X.A\n",
    "M = X.astype(np.bool)\n",
    "theta0 = np.abs(np.random.randn(d.n_users+d.n_movies))\n",
    "theta0 /= np.linalg.norm(theta0)\n",
    "theta = theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.31849837303162\n",
      "114.74017190933228\n",
      "113.59453582763672\n",
      "113.80168032646179\n",
      "113.6618287563324\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sc_times = []\n",
    "sc_solutions = []\n",
    "for i in range(5):\n",
    "    start =  time.time()\n",
    "    sol = optimize.minimize(f, theta, args=(d.n_users,))\n",
    "    elapsed = time.time()-start\n",
    "    print(elapsed)\n",
    "    sc_times.append(elapsed)\n",
    "    sc_solutions.append(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n"
     ]
    }
   ],
   "source": [
    "for sol in sc_solutions:\n",
    "    print(f'MSE: {f(sol.x, d.n_users)/np.count_nonzero(X)}--Norm:{np.linalg.norm(sol.x)}-- n_iter: {sol.nit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.02334303855896"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sc_times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1414.743915881283\t\t4.070705471473702\t\t35308.324169765496\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1335.5743973112167\t\t3.1597194611009\t\t1113.2324279831166\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1293.5715420442891\t\t2.4944886701123883\t\t823.0659234362881\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1267.7710899881586\t\t1.9755038759852241\t\t644.8919070357028\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1251.4339752151802\t\t1.5691338776897972\t\t512.0119973725886\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1240.9400591128315\t\t1.2510357406955925\t\t409.04657103401104\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1234.1099519832194\t\t1.0016810350460703\t\t328.4705099852687\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1229.603297444498\t\t0.8057106493216196\t\t265.0610714699373\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1226.5872723796\t\t0.6512259347156897\t\t214.92692505790217\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1224.539193094674\t\t0.5290585650817687\t\t175.12231739015678\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1223.12753812098\t\t0.43214363305588227\t\t143.39794299249436\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1222.1397349303056\t\t0.35502230506943494\t\t118.0230725555044\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1221.4379651492677\t\t0.2934602872973911\t\t97.65660602027273\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1220.931864645333\t\t0.2441591579438361\t\t81.25295311663625\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1220.5614969452206\t\t0.20453896180259337\t\t67.99275261220659\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1220.2866321322986\t\t0.17257444328570615\t\t57.231353338626526\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1220.079932832693\t\t0.146671300356913\t\t48.460076172728655\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1219.922583758806\t\t0.12557217212199615\t\t41.27675394445392\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1219.8014631774276\t\t0.1082846691908187\t\t35.36307643265353\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1219.7072956948873\t\t0.0940257430382244\t\t30.466985262022213\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.6334347397888\t\t0.08217820999429089\t\t26.38886580746445\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.5750523891725\t\t0.07225640419955702\t\t22.970636196729405\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.5285948349576\t\t0.0638788019717884\t\t20.087081294362527\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[24]\t\t1219.4914125135174\t\t0.05674608697557163\t\t17.638952332188353\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.4615060647334\t\t0.050623556263641296\t\t15.547472361753544\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.4373497981137\t\t0.045327048721322365\t\t13.749970265729612\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.4177675195795\t\t0.04071175629808021\t\t12.196423951004258\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.4018440915524\t\t0.0366633946033201\t\t10.84673540292188\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.3888616427275\t\t0.033091291914011146\t\t9.668592430787484\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.3782529753653\t\t0.029923021831105914\t\t8.635797798965335\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.3695671131395\t\t0.027100263120999824\t\t7.726968018276593\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.3624435239017\t\t0.024575623464351957\t\t6.924522381628779\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3565926172757\t\t0.022310212014555676\t\t6.213898365979904\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.3517808364522\t\t0.020271788190388714\t\t5.582942577237876\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.3478191537704\t\t0.018433350513493645\t\t5.021437221059447\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.3445541169199\t\t0.016772059535009178\t\t4.520730883298225\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.3418608270047\t\t0.01526841338365708\t\t4.073449460708472\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.3396373944242\t\t0.01390561388378639\t\t3.6732686622280992\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.3378005355623\t\t0.012669076323151274\t\t3.314733860190766\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.3362820573827\t\t0.011546047580541638\t\t2.9931164425429357\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3350260382006\t\t0.010525306167352096\t\t2.7042984042467713\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.333986557864\t\t0.00959692440643059\t\t2.444678889041742\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.3331258640212\t\t0.008752077969891997\t\t2.2110978904554006\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.3324128862737\t\t0.00798289172726945\t\t2.000773454438444\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.3318220290803\t\t0.007282313629900371\t\t1.8112495825137092\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.3313321888809\t\t0.006644010418200185\t\t1.6403526812598153\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3309259521836\t\t0.0060622804682877885\t\t1.4861548926741492\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.3305889401431\t\t0.005531980230339908\t\t1.3469430103743651\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.3303092720175\t\t0.0050484615560666005\t\t1.2211919675265541\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.3300771253334\t\t0.004607517842427078\t\t1.107542096709565\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3298843748735\t\t0.0042053373895059405\t\t1.0047795259666255\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.32972429602\t\t0.0038384627240254294\t\t0.9118192016674702\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.3295913207405\t\t0.003503754906501423\t\t0.8276901264573971\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.32948083668\t\t0.003198362042179107\t\t0.7515224771668075\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.329389021609\t\t0.002919691370283015\t\t0.6825363273166747\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3293127068955\t\t0.002665384424611627\t\t0.62003174639618\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.3292492648375\t\t0.002433294850442722\t\t0.5633800860533793\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.3291965156172\t\t0.0022214685344110895\t\t0.5120162937318846\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.329152650422\t\t0.002028125760709084\t\t0.4654321191623571\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3291161678796\t\t0.001851645151858966\t\t0.4231700992200161\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.3290858214796\t\t0.001690549188505978\t\t0.38481822342044014\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.3290605760565\t\t0.0015434911320842482\t\t0.3500051961730881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.3290395717636\t\t0.0014092431981531717\t\t0.3183962234145621\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3290220942267\t\t0.0012866858482217015\t\t0.28968926103487863\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3290075498157\t\t0.0011747980843103715\t\t0.26361167075974995\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3289954451445\t\t0.0010726486447210691\t\t0.23991723611407598\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3289853700715\t\t0.0009793880111453668\t\t0.21838349709414195\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.3289769835992\t\t0.0008942411476056199\t\t0.19880936734460977\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3289700021749\t\t0.0008165009003309263\t\t0.18101300212088242\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.328964189979\t\t0.000745521995443368\t\t0.1648298889769642\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.3289593508644\t\t0.0006807155778214835\t\t0.15011113671628296\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289553216607\t\t0.0006215442403963593\t\t0.13672194077909303\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289519666155\t\t0.0005675174983385588\t\t0.12454020594648804\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.3289491727737\t\t0.0005181876669495291\t\t0.11345530944881858\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.3289468461412\t\t0.0004731461063122474\t\t0.10336698930991736\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.3289449084916\t\t0.0004320197991799386\t\t0.0941843447639227\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.3289432947186\t\t0.00039446823188390295\t\t0.08582493681749458\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.3289419506282\t\t0.0003601805508435974\t\t0.07821397847358169\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.328940831108\t\t0.0003288729699362588\t\t0.07128360527732774\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.3289398986021\t\t0.00030028640614545665\t\t0.06497221787315166\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.328939121842\t\t0.00027418432315747454\t\t0.059223889108458085\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.328938474794\t\t0.0002503507643146626\t\t0.0539878291375651\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.3289379357807\t\t0.00022858855813142568\t\t0.04921790255878014\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289374867509\t\t0.00020871768103578655\t\t0.044872192374806164\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289371126723\t\t0.0001905737634393382\t\t0.04091260600758911\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289368010262\t\t0.00017400672648708285\t\t0.037304519188249986\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.3289365413868\t\t0.000158879537923928\t\t0.03401645392509643\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289363250703\t\t0.00014506707663144143\t\t0.03101978719270143\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.3289361448444\t\t0.0001324550962578746\t\t0.028288487274809575\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.3289359946843\t\t0.00012093927924697354\t\t0.025798875102019437\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.3289358695724\t\t0.00011042437334009575\t\t0.023529408065355327\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.3289357653284\t\t0.00010082340331945655\t\t0.021460484176115766\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.3289356784703\t\t9.205695141543362e-05\t\t0.019574264563301205\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289356060973\t\t8.405250036324094e-05\t\t0.017854512578568692\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.328935545793\t\t7.674383365559603e-05\t\t0.016286447871853054\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.3289354955439\t\t7.007048795867044e-05\t\t0.014856614018350879\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.3289354536732\t\t6.397725317002122e-05\t\t0.013552758416590443\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289354187834\t\t5.8413715990499404e-05\t\t0.012363723259417933\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.3289353897103\t\t5.333384312676592e-05\t\t0.011279346589803707\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289353654836\t\t4.8695600769954164e-05\t\t0.010290372399969973\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.328935345296\t\t4.446060712524186e-05\t\t0.009388369022731378\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.328935328473\t\t4.059381517055905e-05\t\t0.008565654959338706\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.3289353144542\t\t3.70632229423305e-05\t\t0.007815231483978618\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289353027722\t\t3.383960903033448e-05\t\t0.00713072138806667\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.328935293037\t\t3.089629102145743e-05\t\t0.006506313298289602\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289352849245\t\t2.8208904950117712e-05\t\t0.005936711065819532\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.328935278164\t\t2.5755203918678035e-05\t\t0.005417087674407061\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.32893527253\t\t2.351487416157529e-05\t\t0.004943043408501647\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352678348\t\t2.1469367134558473e-05\t\t0.004510567715503841\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.3289352639224\t\t1.960174617471202e-05\t\t0.004116004561664415\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.3289352606619\t\t1.7896546454070693e-05\t\t0.003756020874974574\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352579445\t\t1.6339647149216967e-05\t\t0.0034275778304084584\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352556799\t\t1.4918154638997938e-05\t\t0.003127904730290478\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352537927\t\t1.3620295926779907e-05\t\t0.0028544751761689323\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.3289352522202\t\t1.243532125833377e-05\t\t0.0026049854508832988\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[116]\t\t1219.3289352509096\t\t1.1353415224786354e-05\t\t0.0023773347503179927\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.3289352498173\t\t1.036561558779144e-05\t\t0.002169607237300599\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352489069\t\t9.463739143635738e-06\t\t0.001980055676763503\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.3289352481481\t\t8.640314029424032e-06\t\t0.001807086526831362\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.328935247516\t\t7.888517918012303e-06\t\t0.0016492463544129272\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.3289352469892\t\t7.202121572019126e-06\t\t0.0015052094786480693\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.3289352465501\t\t6.5754373013326284e-06\t\t0.0013737666814440676\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.3289352461843\t\t6.003271913573368e-06\t\t0.001253814945552509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352458792\t\t5.480883710146464e-06\t\t0.0011443480870940428\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.328935245625\t\t5.003943277516855e-06\t\t0.001044448240581498\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.328935245413\t\t4.568497638436607e-06\t\t0.0009532780569216143\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.3289352452366\t\t4.170937534048716e-06\t\t0.0008700736312817397\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.3289352450893\t\t3.8079675519927624e-06\t\t0.0007941380471024823\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.328935244967\t\t3.4765788359354567e-06\t\t0.0007248354725203672\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352448649\t\t3.174024193828676e-06\t\t0.0006615857927782634\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.3289352447796\t\t2.8977953301686877e-06\t\t0.0006038597315137916\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352447086\t\t2.6456020884104663e-06\t\t0.000551174364598489\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.3289352446493\t\t2.4153534895644686e-06\t\t0.0005030890515763928\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.3289352446\t\t2.2051404120898765e-06\t\t0.000459201732406094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352445588\t\t2.0132197747077485e-06\t\t0.0004191455532325633\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352445247\t\t1.8380001063590412e-06\t\t0.00038258575143251816\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352444963\t\t1.678028369601074e-06\t\t0.0003492168566541534\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.3289352444726\t\t1.5319779051818363e-06\t\t0.00031876013122227807\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352444526\t\t1.3986374604814554e-06\t\t0.0002909612016741859\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352444363\t\t1.2769011430510657e-06\t\t0.0002655879552900074\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352444222\t\t1.1657592611847365e-06\t\t0.00024242856103619533\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.3289352444108\t\t1.0642899722033071e-06\t\t0.00022128971285516417\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.3289352444012\t\t9.716515974175743e-07\t\t0.00020199500851295348\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.328935244393\t\t8.870757168521689e-07\t\t0.0001843834451961947\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.3289352443867\t\t8.09860739699285e-07\t\t0.00016830810449103525\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.328935244381\t\t7.393661186914809e-07\t\t0.0001536348996954776\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.3289352443765\t\t6.750070422157113e-07\t\t0.000140241454961745\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.3289352443726\t\t6.162495755417221e-07\t\t0.00012801608964659646\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443694\t\t5.626062407058482e-07\t\t0.0001168568738594573\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.3289352443667\t\t5.136319742281357e-07\t\t0.00010667077944689378\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.3289352443644\t\t4.6892043698615153e-07\t\t9.737289955692449e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.3289352443626\t\t4.2810064853443067e-07\t\t8.888575089868083e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443612\t\t3.908339130945088e-07\t\t8.11386004383878e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.3289352443599\t\t3.5681099428060305e-07\t\t7.406690478650838e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.3289352443587\t\t3.2574958390152706e-07\t\t6.761173766499206e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.328935244358\t\t2.9739191456657674e-07\t\t6.17193323808988e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.3289352443571\t\t2.715026802421592e-07\t\t5.634060288841118e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.3289352443564\t\t2.4786702775373584e-07\t\t5.1430752310100115e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.328935244356\t\t2.2628880875317663e-07\t\t4.694888871200529e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443555\t\t2.0658895430451e-07\t\t4.2857696308166704e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443553\t\t1.8860396103705756e-07\t\t3.912310548212689e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.3289352443549\t\t1.721845739174662e-07\t\t3.571402250806418e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443546\t\t1.5719451497491047e-07\t\t3.260207162565284e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.3289352443544\t\t1.4350937164831713e-07\t\t2.976134140093495e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.3289352443544\t\t1.3101555936783428e-07\t\t2.7168186584883806e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.3289352443542\t\t1.1960937783428803e-07\t\t2.4801022309158788e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.328935244354\t\t1.0919615227507493e-07\t\t2.2640156376342312e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.328935244354\t\t9.968944957652195e-08\t\t2.0667598070742514e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.3289352443537\t\t9.101035769998103e-08\t\t1.8866934695201387e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.328935244354\t\t8.308683533585418e-08\t\t1.7223185537980254e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.3289352443535\t\t7.585310999878134e-08\t\t1.5722662095311685e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.3289352443537\t\t6.92491350906397e-08\t\t1.4352898913022215e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.3289352443537\t\t6.322008871091877e-08\t\t1.310248438739087e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.3289352443537\t\t5.771592462627424e-08\t\t1.1961024797714503e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443535\t\t5.26909464644128e-08\t\t1.091902343968462e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.3289352443535\t\t4.8103441269479346e-08\t\t9.967812297752987e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443535\t\t4.3915325435209116e-08\t\t9.0994717606705e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t4.009183233587303e-08\t\t8.306792769081186e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t3.6601213598565344e-08\t\t7.583170103178329e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443535\t\t3.341450013503113e-08\t\t6.922598729105227e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t3.05052255874571e-08\t\t6.319572584924088e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t2.7849238795177812e-08\t\t5.7690820835013875e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t2.5424493508632076e-08\t\t5.266550615975373e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t2.3210854293997352e-08\t\t4.8077973768787465e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t2.118994202768038e-08\t\t4.389007525138548e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t1.934497899078686e-08\t\t4.006707710180451e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443535\t\t1.766064971217245e-08\t\t3.65770597464546e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t1.612296359019743e-08\t\t3.339106511438992e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443533\t\t1.471915977412279e-08\t\t3.048266399637216e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.3437577299197053e-08\t\t2.7827562907573456e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443535\t\t1.2267578409164598e-08\t\t2.5403743710344465e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[192]\t\t1219.3289352443535\t\t1.119944675210339e-08\t\t2.319108450468212e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[193]\t\t1219.3289352443535\t\t1.0224314114584262e-08\t\t2.1171139579039935e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[194]\t\t1219.3289352443533\t\t9.334084356159851e-09\t\t1.932716045468945e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.010935653116285186s\n",
      "Estimated average function evaluation times: 0.00036655013094243316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1587.9040873735098\t\t5.6150207448141956\t\t36147.361565807594\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1363.8639764173981\t\t3.6109671928862817\t\t2287.9220985971197\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1287.096416809854\t\t2.341879202421438\t\t1140.0373329079162\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1258.0979427428988\t\t1.6205068418080237\t\t635.4774530776261\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1244.211831303864\t\t1.1848794664752538\t\t422.96628858966847\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1236.313716706127\t\t0.8988084709524679\t\t313.78748034012074\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1231.396600196608\t\t0.7006541750212881\t\t242.79703506141863\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1228.1662459347785\t\t0.5590464667131357\t\t191.29616758386487\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1225.95691971031\t\t0.45555130462690185\t\t152.67641814719533\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1224.3945951888138\t\t0.37834461262677194\t\t123.38911661346005\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1223.2584025903907\t\t0.3195281965960806\t\t100.99069319940939\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1222.4127884937\t\t0.2737385221218598\t\t83.68662293745663\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1221.7715310466124\t\t0.23730835618514887\t\t70.15679583681717\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1221.2778730562375\t\t0.20771852111342598\t\t59.43830868826344\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1220.893234425148\t\t0.1832274663926931\t\t50.83252289702965\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1220.5906203769255\t\t0.16261946664429702\t\t43.832506967484946\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1220.350659130122\t\t0.14503433078867561\t\t38.06864141696775\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1220.1591470304306\t\t0.1298530882574558\t\t33.269130464397634\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1220.0054804077743\t\t0.11662165827387308\t\t29.231923803616016\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1219.881623652506\t\t0.1049999526727289\t\t25.804970796776466\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.7814110065283\t\t0.09472783870354748\t\t22.872381457702627\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.7000620931312\t\t0.08560221864616081\t\t20.344703541384963\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.6338381814928\t\t0.0774614351837201\t\t18.15204561246218\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[24]\t\t1219.5797935347262\t\t0.07017452223462783\t\t16.239166393916165\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.5355925161034\t\t0.06363368415056304\t\t14.561928704198976\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.49937312568\t\t0.057748947744951276\t\t13.084708180891216\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.469643922047\t\t0.052444294723186596\t\t11.778477125816416\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.4452053324517\t\t0.047654816636331886\t\t10.619371359788692\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.42508902611\t\t0.043324586454481055\t\t9.58760683183978\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.4085108276267\t\t0.03940503994738032\t\t8.666652476984888\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.394833887342\t\t0.035853725237686054\t\t7.8425928921975325\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.383539693886\t\t0.032633322217491066\t\t7.103633050921091\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3742051321465\t\t0.029710862657729622\t\t6.439710286358268\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.366484235779\t\t0.027057101682739185\t\t5.842187967561068\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.360093609217\t\t0.02464600496661378\t\t5.30361187217057\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.354800734902\t\t0.022454325565000134\t\t4.81751502390862\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.350414561146\t\t0.020461251053170555\t\t4.378260249307977\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.3467779014013\t\t0.018648106475980076\t\t3.98091228356011\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.3437612785024\t\t0.016998102114236253\t\t3.6211331731716\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.3412579261126\t\t0.015496117632070988\t\t3.2950961618560406\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3391797202257\t\t0.014128516062783623\t\t2.999414332344817\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.337453860582\t\t0.01288298250449584\t\t2.731081101544587\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.3360201585283\t\t0.01174838346355494\t\t2.487420296358484\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.3348288166014\t\t0.01071464359577356\t\t2.266044020817402\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.3338386077978\t\t0.009772637220296707\t\t2.064816897856901\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.3330153804195\t\t0.008914092465403665\t\t1.881825558051524\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3323308286754\t\t0.008131506285194682\t\t1.71535247266867\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.331761480592\t\t0.00741806888609513\t\t1.563853404304753\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.3312878639215\t\t0.006767596341197909\t\t1.4259378871013335\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.3308938180692\t\t0.006174470362935527\t\t1.3003522577223814\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3305659259609\t\t0.005633584360542811\t\t1.1859648454077991\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.3302930445702\t\t0.005140295036337976\t\t1.0817529987166135\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.330065916678\t\t0.004690378880100515\t\t0.9867916822242943\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.3298768495943\t\t0.004279993008120139\t\t0.9002434214966397\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.3297194491277\t\t0.0039056398665181097\t\t0.8213494107910903\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3295883991793\t\t0.0035641353800368496\t\t0.749421627880238\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.3294792790427\t\t0.0032525801795694827\t\t0.6838358244369229\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.3293884118884\t\t0.0029683335861577977\t\t0.6240252804449853\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.3293127390543\t\t0.0027089900673164936\t\t0.5694752276588532\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3292497157122\t\t0.002472357914306747\t\t0.5197178608075\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.329197224237\t\t0.0022564399174724407\t\t0.4743278666098186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.3291535022581\t\t0.0020594158413352157\t\t0.43291841041768947\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.3291170828822\t\t0.0018796265228890625\t\t0.3951375281858598\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3290867450196\t\t0.001715559435205691\t\t0.36066487846123274\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3290614720916\t\t0.001565835575246295\t\t0.32920881479894326\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3290404177003\t\t0.001429197549319226\t\t0.30050374399352875\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3290228770793\t\t0.0013044987426150036\t\t0.27430773967887107\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.329008263349\t\t0.0011906934706886647\t\t0.25040038463883213\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3289960877623\t\t0.0010868280209873796\t\t0.2285808181398516\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.3289859432712\t\t0.0009920325015924216\t\t0.20866596743497642\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.3289774908476\t\t0.0009055134224810296\t\t0.19048894487495996\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289704481022\t\t0.0008265469417624467\t\t0.17389759420844636\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289645798122\t\t0.0007544727159897135\t\t0.15875317130123187\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.3289596900327\t\t0.0006886882993028468\t\t0.14492914631760856\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.328955615531\t\t0.0006286440415077836\t\t0.13231011546584476\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.3289522203222\t\t0.0005738384398363581\t\t0.12079081197767541\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.3289493911152\t\t0.0005238139033755206\t\t0.11027520683277656\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.328947033523\t\t0.0004781528929637612\t\t0.1006756907664773\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.3289450689058\t\t0.00043647440286601806\t\t0.09191232997605803\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.3289434317426\t\t0.00039843075347574915\t\t0.08391218867759309\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.3289420674412\t\t0.0003637046673132748\t\t0.07660871230150597\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.3289409305137\t\t0.0003320066029924814\t\t0.069941165772788\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.3289399830578\t\t0.0003030723241632394\t\t0.06385412185362314\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289391934916\t\t0.00027666068253945266\t\t0.05829699492139053\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289385354988\t\t0.0002525515959991648\t\t0.053223616150074066\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289379871503\t\t0.00023054420445984118\t\t0.04859184625986717\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.3289375301729\t\t0.00021045518776856235\t\t0.044363222536824475\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289371493395\t\t0.0001921172313114464\t\t0.0405026369827745\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.3289368319606\t\t0.00017537762627317562\t\t0.03697804282514361\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.3289365674625\t\t0.00016009699264368443\t\t0.03376018687301543\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.3289363470333\t\t0.000146148114199555\t\t0.030822365403174498\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.3289361633297\t\t0.0001334148755481973\t\t0.028140201478868476\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.3289360102326\t\t0.0001217912922801743\t\t0.025691441808385448\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289358826419\t\t0.00011118062604663194\t\t0.02345577141813699\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.3289357763083\t\t0.0001014945770641369\t\t0.021414644539729112\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.3289356876899\t\t9.265254728471353e-05\t\t0.019551130334599303\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.3289356138348\t\t8.458096800569338e-05\t\t0.01784977207886286\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289355522838\t\t7.721268628833332e-05\t\t0.016296458686926626\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.3289355009865\t\t7.048640498119997e-05\t\t0.014878307427261693\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289354582353\t\t6.434617173033013e-05\t\t0.013583556926412452\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.3289354226058\t\t5.8740912591817676e-05\t\t0.012401469465445894\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.328935392912\t\t5.3624006433753026e-05\t\t0.011322241844354338\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.3289353681646\t\t4.895289644987013e-05\t\t0.010336923999157642\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289353475398\t\t4.468873566737873e-05\t\t0.00943734470296548\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.3289353303508\t\t4.079606335415914e-05\t\t0.008616043782431741\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289353160255\t\t3.724250970553975e-05\t\t0.007866210208571759\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.3289353040864\t\t3.399852627287569e-05\t\t0.007181625604777888\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.328935294136\t\t3.103713997744802e-05\t\t0.00655661264440398\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352858433\t\t2.8333728523142415e-05\t\t0.005985987994551979\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.328935278932\t\t2.586581544963295e-05\t\t0.005465019294960893\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.3289352731717\t\t2.361288304811174e-05\t\t0.004989385938366978\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352683714\t\t2.1556201609093405e-05\t\t0.004555143224881893\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352643706\t\t1.967867353423527e-05\t\t0.004158689682642769\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352610361\t\t1.7964691109921102e-05\t\t0.0037967371584000163\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.3289352582572\t\t1.6400006586970023e-05\t\t0.0034662836043275005\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[116]\t\t1219.3289352559411\t\t1.497161369557664e-05\t\t0.003164588154616523\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.328935254011\t\t1.3667639408902005e-05\t\t0.0028891484145156725\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352524023\t\t1.247724519042462e-05\t\t0.0026376797084476915\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.3289352510615\t\t1.1390536835388432e-05\t\t0.0024080961439752542\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.328935249944\t\t1.0398482173328189e-05\t\t0.002198493312812195\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.3289352490128\t\t9.4928359437606e-06\t\t0.0020071325063766997\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.3289352482366\t\t8.66607123194622e-06\t\t0.001832426292000003\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.32893524759\t\t7.911316870912819e-06\t\t0.0016729253629585055\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352470508\t\t7.222300288616777e-06\t\t0.0015273065170911915\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.3289352466013\t\t6.593295375753314e-06\t\t0.0013943616762641434\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.328935246227\t\t6.019074838594123e-06\t\t0.0012729878952035922\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.3289352459149\t\t5.4948667626727805e-06\t\t0.0011621781858066498\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.3289352456547\t\t5.016314936267119e-06\t\t0.001061013190800186\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.328935245438\t\t4.579442617118226e-06\t\t0.0009686535386549452\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352452575\t\t4.1806194669551105e-06\t\t0.0008843329156593834\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.3289352451065\t\t3.816531405566857e-06\t\t0.0008073516704746004\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352449813\t\t3.484153020697232e-06\t\t0.0007370710521502914\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.328935244877\t\t3.1807224376731344e-06\t\t0.0006729078819607487\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.3289352447896\t\t2.9037183635303854e-06\t\t0.0006143297404972735\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352447168\t\t2.6508391124605247e-06\t\t0.0005608505292435434\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352446563\t\t2.4199834967816238e-06\t\t0.000512026463805966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352446059\t\t2.2092333378885423e-06\t\t0.00046745236925058696\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.3289352445638\t\t2.0168375498558345e-06\t\t0.0004267583390491179\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352445286\t\t1.8411975498470356e-06\t\t0.0003896066582135334\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352444995\t\t1.6808540085461722e-06\t\t0.00035568899588507097\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352444754\t\t1.5344746964793472e-06\t\t0.0003247238602123239\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.328935244455\t\t1.4008434250273364e-06\t\t0.0002964542616921897\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.328935244438\t\t1.2788499274969012e-06\t\t0.0002706455712349388\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.328935244424\t\t1.1674806468260015e-06\t\t0.00024708358048677014\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.3289352444122\t\t1.0658102935244663e-06\t\t0.0002255727273985893\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.3289352444026\t\t9.72994181347115e-07\t\t0.00020593446845172415\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.3289352443942\t\t8.882611830483349e-07\t\t0.00018800579909010996\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.3289352443876\t\t8.109073350541489e-07\t\t0.00017163790850415836\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443817\t\t7.402899880294081e-07\t\t0.00015669493731675224\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.328935244377\t\t6.758224590271083e-07\t\t0.00014305283495017458\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.328935244373\t\t6.169691671598192e-07\t\t0.00013059836866824925\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.3289352443699\t\t5.632411687515924e-07\t\t0.00011922816263559968\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443671\t\t5.141921141505726e-07\t\t0.00010884781489404094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.3289352443649\t\t4.6941451786871645e-07\t\t9.937116714631574e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.328935244363\t\t4.285363899849263e-07\t\t9.071953977866988e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.3289352443612\t\t3.9121813547539003e-07\t\t8.282112649236918e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.3289352443599\t\t3.5714972925169855e-07\t\t7.561035103530014e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.328935244359\t\t3.2604815841241354e-07\t\t6.90273428562856e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.328935244358\t\t2.9765504468259495e-07\t\t6.301745831695673e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443571\t\t2.717345238758993e-07\t\t5.753080899991586e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443567\t\t2.4807126142516754e-07\t\t5.2521830113980065e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.328935244356\t\t2.2646868653907687e-07\t\t4.794895016064134e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443555\t\t2.0674733996767572e-07\t\t4.3774196796916746e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.328935244355\t\t1.8874339772515327e-07\t\t3.9962911516091415e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.328935244355\t\t1.723072945224541e-07\t\t3.648344716690603e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.3289352443546\t\t1.5730249449607412e-07\t\t3.330692543800202e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.3289352443544\t\t1.4360435920336403e-07\t\t3.0406959303588877e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.3289352443544\t\t1.310990945246563e-07\t\t2.7759483893026878e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.3289352443542\t\t1.1968281917053135e-07\t\t2.5342508369166278e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.328935244354\t\t1.0926070064252763e-07\t\t2.3135971353688555e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.328935244354\t\t9.974616582509889e-08\t\t2.112154752583847e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.3289352443537\t\t9.106017300233704e-08\t\t1.92825095557558e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.3289352443537\t\t8.313057618549784e-08\t\t1.7603591960311702e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.3289352443537\t\t7.589150406259081e-08\t\t1.6070852995690212e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443537\t\t6.928282264762194e-08\t\t1.4671564528777123e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.328935244354\t\t6.324963772318528e-08\t\t1.3394107372107413e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443535\t\t5.774182525364412e-08\t\t1.222787627900786e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t5.271364644288734e-08\t\t1.1163188191971626e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t4.812332620496127e-08\t\t1.0191202452821234e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443535\t\t4.393273567716555e-08\t\t9.303843525201703e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t4.010707082805452e-08\t\t8.493745089133315e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t3.661454121445218e-08\t\t7.754183071240132e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t3.3426148249139606e-08\t\t7.0790132646091075e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t3.051539999191147e-08\t\t6.462632393142398e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t2.785812322258029e-08\t\t5.899918966893741e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t2.5432243820859037e-08\t\t5.386200218006816e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443535\t\t2.3217611045684307e-08\t\t4.917212669852743e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t2.1195830040359468e-08\t\t4.489056942644734e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443535\t\t1.9350108449066573e-08\t\t4.098182402058644e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.7665107770829824e-08\t\t3.7413428276666485e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443535\t\t1.6126837328376198e-08\t\t3.415575805148904e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[192]\t\t1219.3289352443535\t\t1.4722523546080772e-08\t\t3.118168880492343e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[193]\t\t1219.3289352443535\t\t1.344049703377484e-08\t\t2.8466628577084825e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[194]\t\t1219.3289352443535\t\t1.2270106463919706e-08\t\t2.598793663957595e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[195]\t\t1219.3289352443535\t\t1.1201634491835697e-08\t\t2.372506953916556e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[196]\t\t1219.3289352443535\t\t1.0226205913562979e-08\t\t2.165925062413196e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[197]\t\t1219.3289352443535\t\t9.335713507573227e-09\t\t1.9773305714647112e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.010573964433621633s\n",
      "Estimated average function evaluation times: 0.0003729907389219642\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1464.1311621395325\t\t4.2631569559071165\t\t36378.8164344041\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1359.2479584038067\t\t3.3551322309026617\t\t1451.8413000122675\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1302.6792385566912\t\t2.632777463365033\t\t1047.6181742420663\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1269.7353277391949\t\t2.0517150612279003\t\t784.3788924244445\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1250.1923974218294\t\t1.5945447626000324\t\t597.0015497853398\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1238.4873884807662\t\t1.2403213221785843\t\t458.77659843020507\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1231.4073554577021\t\t0.9679356803986366\t\t355.0531709873305\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1227.0756051249487\t\t0.7589516207203864\t\t276.3878023399818\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1224.3910116692068\t\t0.598445027946846\t\t216.25039437510506\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1222.703673616236\t\t0.47480999817985814\t\t169.9792491273626\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1221.6269147545672\t\t0.37920545029593317\t\t134.1850114904131\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1220.9284778813765\t\t0.3049592467483768\t\t106.3703180018599\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1220.4674159020285\t\t0.24704868596436047\t\t84.67468739461465\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1220.1572774801302\t\t0.2016860159026301\t\t67.69885174045908\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1219.9444577779595\t\t0.16600221770226692\t\t54.38155287803937\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1219.795348522077\t\t0.13781152683219727\t\t43.91182547774188\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1219.6886362296843\t\t0.11543833980978738\t\t35.665733135021064\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1219.6106409343083\t\t0.09759073480460884\t\t29.16023473631213\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1219.5524683106792\t\t0.08326809791244685\t\t24.019235938102845\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1219.5082538783784\t\t0.07169331833891054\t\t19.94843188787717\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.474071038491\t\t0.06226246101956451\t\t16.716570793915867\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.4472461902271\t\t0.054506775852441104\t\t14.141445285988759\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.42592554341\t\t0.04806343813307535\t\t12.079367668624238\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[24]\t\t1219.408798762281\t\t0.04265258828526444\t\t10.417190511072135\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.3949210535216\t\t0.03805908308133463\t\t9.066158379395851\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.383597494095\t\t0.034117925867622655\t\t7.957060766051644\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.3743069948487\t\t0.030702672366765094\t\t7.036316649348163\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.3666516962364\t\t0.027716286088375675\t\t6.2627543386224565\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.3603228161173\t\t0.025084010898568924\t\t5.604946933747259\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.355077237503\t\t0.022747885530504268\t\t5.039020964061771\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.350721180075\t\t0.020662571643503134\t\t4.546880919044086\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.347098599541\t\t0.018792213210631226\t\t4.114798409281281\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3440827854306\t\t0.017108091489291665\t\t3.7323136146493576\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.3415701560493\t\t0.015586884352942355\t\t3.3913959074750952\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.339475588614\t\t0.014209378921962274\t\t3.085812858715196\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.3377288418687\t\t0.01295952078319691\t\t2.8106621060883885\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.3362717712043\t\t0.011823711241068957\t\t2.56202751579348\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.335056129913\t\t0.010790286358722685\t\t2.336728401426661\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.3340418121884\t\t0.009849128785918865\t\t2.1321373759595503\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.33319543501\t\t0.008991376426498136\t\t1.9460482483849115\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3324891841842\t\t0.00820920173828627\t\t1.7765801061842197\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.3318998692266\t\t0.007495642642643009\t\t1.6221074107411144\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.331408145325\t\t0.0068444712737011785\t\t1.4812087252625235\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.3309978703203\t\t0.00625009061439056\t\t1.3526287668514325\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.3306555716708\t\t0.005707451826143356\t\t1.2352499890472828\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.330370003589\t\t0.005211987068981557\t\t1.1280709952922463\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3301317784783\t\t0.004759554039548959\t\t1.0301898682920898\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.3299330598272\t\t0.004346389482898715\t\t0.9407910586006901\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.3297673060904\t\t0.003969069672229694\t\t0.8591348719204521\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.329629056962\t\t0.0036244763814960545\t\t0.7845488743100545\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3295137549494\t\t0.0033097672576602945\t\t0.7164207319257514\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.3294175963802\t\t0.003022349774687326\t\t0.6541921407062088\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.3293374069717\t\t0.002759858150629775\t\t0.5973535993084663\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.3292705379008\t\t0.0025201327537762003\t\t0.5454398471255731\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.3292147790037\t\t0.0023012016294329096\t\t0.49802583786519056\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3291682862787\t\t0.002101263856552287\t\t0.4547231531748796\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.3291295213437\t\t0.0019186745009886117\t\t0.4151767848627105\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.32909720088\t\t0.0017519309750794337\t\t0.3790622315699881\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.3290702544182\t\t0.0015996606458419632\t\t0.34608286777275027\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3290477890946\t\t0.0014606095590793114\t\t0.31596755191506576\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.3290290602322\t\t0.001333632165994216\t\t0.28846844670621813\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.3290134467813\t\t0.0012176819543257436\t\t0.263359029453951\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.3290004308244\t\t0.0011118028982625959\t\t0.24043227360056593\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3289895804728\t\t0.0010151216515630744\t\t0.2194989854957208\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3289805355914\t\t0.0009268404164439006\t\t0.20038628225660532\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3289729958929\t\t0.0008462304281537848\t\t0.18293619842370296\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3289667110016\t\t0.0007726260008742853\t\t0.16700441028339866\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.3289614721643\t\t0.000705419085982502\t\t0.1524590678852052\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3289571053378\t\t0.0006440542983012386\t\t0.13917972572360224\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.3289534654214\t\t0.0005880243698869577\t\t0.12705636383635402\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.3289504314482\t\t0.0005368659946217127\t\t0.1159884918403572\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289479025725\t\t0.0004901560300699028\t\t0.10588432892285919\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289457947265\t\t0.00044750802602866945\t\t0.09666005357921334\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.3289440378292\t\t0.00040856905173032663\t\t0.08823911719004753\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.3289425734604\t\t0.0003730167962438803\t\t0.08055161619599105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.3289413529228\t\t0.00034055691857672775\t\t0.07353371790437552\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.328940335623\t\t0.00031092062619926455\t\t0.06712713544505812\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.3289394877238\t\t0.00028386246235244297\t\t0.061278647789078794\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.3289387810212\t\t0.0002591582843138906\t\t0.05593966094563956\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.3289381920051\t\t0.00023660341616945252\t\t0.05106580692162459\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.3289377010797\t\t0.00021601096116189371\t\t0.046616577209336034\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.3289372919116\t\t0.000197210259896891\t\t0.04255498789500481\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.328936950886\t\t0.00018004548186355317\t\t0.03884727363869939\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289366666559\t\t0.00016437433878761635\t\t0.035462608134255835\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289364297632\t\t0.00015006690935393872\t\t0.03237284873112126\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289362323242\t\t0.00013700456567063423\t\t0.029552303136136635\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.328936067769\t\t0.00012507899274078984\t\t0.02697751639200776\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289359306207\t\t0.0001141912928673307\t\t0.024627076238122048\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.3289358163152\t\t0.00010425116770140547\t\t0.02248143540953914\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.328935721048\t\t9.517617121760091e-05\t\t0.02052274935539867\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.3289356416483\t\t8.689102744228523e-05\t\t0.01873472802606933\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.3289355754732\t\t7.932700743789841e-05\t\t0.017102500538779257\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.32893552032\t\t7.24213602537892e-05\t\t0.015612491644393343\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289354743538\t\t6.611679337976351e-05\t\t0.014252308865411097\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.3289354360431\t\t6.036099820221919e-05\t\t0.013010639496912047\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.328935404114\t\t5.5106216741234705e-05\t\t0.011877156579598471\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.328935377503\t\t5.0308845959245695e-05\t\t0.010842433002993706\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289353553248\t\t4.592907649332584e-05\t\t0.00989786313023019\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.3289353368402\t\t4.1930562728550464e-05\t\t0.00903559122213613\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289353214348\t\t3.82801215584697e-05\t\t0.008248446096836996\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.3289353085956\t\t3.494745733985241e-05\t\t0.007529881493124837\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.3289352978948\t\t3.1904910719032175e-05\t\t0.006873921611302324\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.3289352889765\t\t2.9127229313859572e-05\t\t0.006275111393999401\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289352815436\t\t2.6591358326993135e-05\t\t0.005728471166128059\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.3289352753486\t\t2.427624936040362e-05\t\t0.005229455189267077\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289352701859\t\t2.2162685857171094e-05\t\t0.004773913852786758\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.328935265883\t\t2.0233123724034335e-05\t\t0.004358059165433112\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.3289352622967\t\t1.8471545802846505e-05\t\t0.003978433213846071\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352593079\t\t1.6863329008584626e-05\t\t0.003631879417429444\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.328935256817\t\t1.5395123035015353e-05\t\t0.0033155162695836784\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.3289352547408\t\t1.4054739581718011e-05\t\t0.0030267133349018717\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352530105\t\t1.283105124208223e-05\t\t0.0027630694093603893\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352515685\t\t1.1713899179204406e-05\t\t0.0025223925185270837\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352503666\t\t1.069400882627738e-05\t\t0.002302681683344412\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.328935249365\t\t9.762912940276808e-06\t\t0.002102110291479684\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[116]\t\t1219.3289352485303\t\t8.91288132228689e-06\t\t0.0019190108749675742\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.3289352478346\t\t8.136856693360645e-06\t\t0.0017518612743306385\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352472548\t\t7.428396099056309e-06\t\t0.0015992719471217016\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.3289352467714\t\t6.781617451055069e-06\t\t0.0014599744251459548\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.3289352463687\t\t6.191150722405148e-06\t\t0.0013328107677418257\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.3289352460329\t\t5.652093351843291e-06\t\t0.0012167239126307846\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.328935245753\t\t5.159969577873856e-06\t\t0.001110748899063105\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.3289352455201\t\t4.710693264659666e-06\t\t0.0010140048513404523\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352453257\t\t4.300534010575243e-06\t\t0.0009256876240476273\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.3289352451638\t\t3.926086170535193e-06\t\t0.0008450631348913341\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.3289352450288\t\t3.5842405756883966e-06\t\t0.0007714612670520292\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.3289352449165\t\t3.2721587452839e-06\t\t0.0007042702708680921\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.3289352448226\t\t2.9872493167269746e-06\t\t0.0006429317011253802\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.3289352447443\t\t2.727146521707386e-06\t\t0.0005869357588954097\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352446793\t\t2.489690567091271e-06\t\t0.0005358170690912664\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.328935244625\t\t2.2729096868748043e-06\t\t0.000489150778052701\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352445797\t\t2.075003776350979e-06\t\t0.00044654906032282467\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.3289352445422\t\t1.8943294598168634e-06\t\t0.0004076578744811576\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.3289352445106\t\t1.7293864321123594e-06\t\t0.0003721540083237992\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352444845\t\t1.5788050188831888e-06\t\t0.000339742423150694\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352444626\t\t1.4413347850089258e-06\t\t0.0003101537623969898\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352444444\t\t1.3158341649923224e-06\t\t0.00028314215440588696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.3289352444292\t\t1.2012609854483914e-06\t\t0.0002584831263184965\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352444167\t\t1.0966638161388787e-06\t\t0.00023597176579285724\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352444062\t\t1.00117405136407e-06\t\t0.00021542101880653671\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352443974\t\t9.139987175617214e-07\t\t0.00019666011465842095\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.32893524439\t\t8.344138835577606e-07\t\t0.00017953315344352045\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.3289352443837\t\t7.617586460771527e-07\t\t0.00016389782646191573\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.328935244379\t\t6.954296457553519e-07\t\t0.00014962421645238353\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.3289352443746\t\t6.348760555594786e-07\t\t0.00013659372375575076\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.3289352443712\t\t5.795950098300095e-07\t\t0.00012469807380703323\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.328935244368\t\t5.291274263563524e-07\t\t0.00011383843014143005\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.3289352443658\t\t4.830541879672712e-07\t\t0.00010392455954422611\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443635\t\t4.409926792823788e-07\t\t9.487408636642402e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.3289352443621\t\t4.025935876849166e-07\t\t8.661182108953603e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.3289352443605\t\t3.675380239820128e-07\t\t7.906911332934932e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.3289352443594\t\t3.355348647915071e-07\t\t7.21832895539091e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443583\t\t3.0631832861886543e-07\t\t6.589714800044722e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.3289352443576\t\t2.796457767864497e-07\t\t6.015845455388425e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.328935244357\t\t2.5529570748619597e-07\t\t5.4919532671126324e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.3289352443562\t\t2.3306588800000087e-07\t\t5.013685887153158e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.3289352443558\t\t2.127717021754704e-07\t\t4.577069434641557e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.3289352443553\t\t1.9424461304170552e-07\t\t4.178476861019813e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.3289352443549\t\t1.773307558935234e-07\t\t3.814596253796887e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443549\t\t1.6188965987617401e-07\t\t3.4824048193688004e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443546\t\t1.4779308747293434e-07\t\t3.179143266927341e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.3289352443544\t\t1.3492396094065736e-07\t\t2.902290884831541e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443542\t\t1.2317540990806119e-07\t\t2.649549025929645e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.3289352443542\t\t1.1244986228968545e-07\t\t2.4188167387548127e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.328935244354\t\t1.0265823730619991e-07\t\t2.208178020021188e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.328935244354\t\t9.371921373906455e-08\t\t2.01588284372284e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.3289352443537\t\t8.555855346722802e-08\t\t1.840333804703386e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.328935244354\t\t7.810849010826687e-08\t\t1.6800725134742323e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.328935244354\t\t7.130713414996407e-08\t\t1.5337669756982248e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.3289352443537\t\t6.509801266710616e-08\t\t1.4002027466510155e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.3289352443535\t\t5.9429543817444253e-08\t\t1.2782698970478773e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.3289352443535\t\t5.425466311731356e-08\t\t1.1669553308428055e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.3289352443535\t\t4.953038983100306e-08\t\t1.065334259732888e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.3289352443535\t\t4.52174803524662e-08\t\t9.725630167156515e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443535\t\t4.128012061767219e-08\t\t8.878701256208994e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.3289352443535\t\t3.7685607522676365e-08\t\t8.105533459059954e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443535\t\t3.440408788163107e-08\t\t7.399689327332079e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t3.1408316023435344e-08\t\t6.755315914135905e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t2.86733947011264e-08\t\t6.167051281093091e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443535\t\t2.617662278090709e-08\t\t5.630017304059266e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t2.3897258435915228e-08\t\t5.139748707513908e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t2.1816372012140495e-08\t\t4.692173991871314e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t1.9916681014080136e-08\t\t4.283574250759746e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t1.81824070600661e-08\t\t3.910557877355749e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t1.659914735550302e-08\t\t3.5700234471844653e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t1.5153752698997404e-08\t\t3.2591415087320945e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443535\t\t1.383421897686167e-08\t\t2.975335863870805e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t1.2629580202795915e-08\t\t2.7162396861873223e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443535\t\t1.1529839383917172e-08\t\t2.4797103606428883e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.0525858145520196e-08\t\t2.2637787188441466e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443535\t\t9.609302609683966e-09\t\t2.0666465882122858e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.01064374808865692s\n",
      "Estimated average function evaluation times: 0.00034322414098609806\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1326.2046835173796\t\t2.147455171768096\t\t35895.30974070825\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1285.2686366592084\t\t1.5825021917772744\t\t725.9771992752751\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1264.8674998863962\t\t1.234786048456747\t\t517.6893998751456\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1252.1109744845112\t\t0.9859004607943511\t\t403.38000873274115\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1243.4714256788936\t\t0.7999594860239836\t\t324.30171730776567\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1237.411260165662\t\t0.6588359777860981\t\t264.6467614510013\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1233.0623810777988\t\t0.5504389404226429\t\t218.25979428892177\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1229.8835569001294\t\t0.46608572669889603\t\t181.67573404427327\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1227.5227657423834\t\t0.399470981915793\t\t152.52987210102359\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1225.7450021660256\t\t0.3460259542648379\t\t129.09423276748493\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1224.3900016728587\t\t0.3024507474694805\t\t110.07803258947212\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1223.3463371016906\t\t0.26636398330885463\t\t94.50848761767405\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1222.5351379285387\t\t0.23604297636968904\t\t81.64811533742579\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1221.89965010276\t\t0.21023417950065176\t\t70.93464603542485\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1221.398415108868\t\t0.18801701404587795\t\t61.936831621557175\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1221.0007324485732\t\t0.16870723765162682\t\t54.32181246817771\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1220.6835880066928\t\t0.15178894047804672\t\t47.83094301502292\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1220.429539708691\t\t0.1368669031176733\t\t42.261820576084425\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1220.22523889329\t\t0.12363325041038264\t\t37.45487515166425\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1220.0603807525204\t\t0.11184405900426889\t\t33.283324731210854\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.9269489079236\t\t0.1013028654541786\t\t29.64562362723233\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.8186646358508\t\t0.09184895367858541\t\t26.459764427226176\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.7305804930636\t\t0.08334895878166802\t\t23.658962686045097\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\t\t1219.6587771878712\t\t0.07569078167284357\t\t21.1883760407759\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.6001351930681\t\t0.06877912361353973\t\t19.00259914083978\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.5521610968858\t\t0.06253216510482196\t\t17.06374185249237\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.5128544762965\t\t0.05687906047273017\t\t15.339947088384342\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.4806050696504\t\t0.05175801983678936\t\t13.804240920646572\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.4541128139904\t\t0.04711481880437311\t\t12.433634659135711\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.4323252829238\t\t0.04290162339399697\t\t11.208418714865925\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.4143884693483\t\t0.039076050244688375\t\t10.111603087188842\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.3996078749092\t\t0.0356004047714569\t\t9.128470517671294\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3874176108227\t\t0.03244105572438121\t\t8.24621671595772\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.3773557621632\t\t0.02956791573049865\t\t7.453658307494176\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.369044674896\t\t0.026954005289366766\t\t6.740993820056154\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.3621751304245\t\t0.024575083340469043\t\t6.099606519252404\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.3564936034059\t\t0.02240933159858199\t\t5.521900521825547\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.3517919745775\t\t0.02043708282595546\t\t5.001163584459461\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.3478992053033\t\t0.018640585399395414\t\t4.531451449536855\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.3446745847518\t\t0.017003798159437852\t\t4.10748975278167\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3420022414987\t\t0.015512210754266564\t\t3.72459035111663\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.3397866744895\t\t0.014152685623462375\t\t3.3785795814185464\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.337949107812\t\t0.012913318483771119\t\t3.065736461863093\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.3364245127464\t\t0.011783314737146479\t\t2.7827392350061197\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.3351591714215\t\t0.010752879660338896\t\t2.526618953403645\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.3341086809087\t\t0.009813120584312584\t\t2.2947190450170094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3332363161144\t\t0.00895595955228064\t\t2.0846599826304595\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.332511685439\t\t0.008174055172749285\t\t1.8943083302876516\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.3319096256757\t\t0.0074607325702816234\t\t1.721749559307242\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.3314092926885\t\t0.006809920490734215\t\t1.5652641230074442\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3309934124995\t\t0.006216094745701201\t\t1.4233063582149164\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.3306476639746\t\t0.0056742272886189694\t\t1.2944858462730515\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.3303601695798\t\t0.005179740305550278\t\t1.1775509199008507\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.3301210749867\t\t0.004728464780789223\t\t1.071374046841864\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.3299222017881\t\t0.004316603063266341\t\t0.9749388586159633\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3297567604332\t\t0.0039406950162821424\t\t0.887328624239466\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.329619112797\t\t0.003597587381702545\t\t0.8077159953913722\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.3295045757015\t\t0.0032844060320237948\t\t0.7353538723313083\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.3294092582375\t\t0.0029985308204226136\t\t0.6695672591420263\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3293299270135\t\t0.0027375727707894624\t\t0.6097459937375184\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.3292638944877\t\t0.0024993533778479315\t\t0.5553382521968901\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.3292089263905\t\t0.0022818858119994013\t\t0.505844739507594\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.3291631649463\t\t0.0020833578451336034\t\t0.4608134895697669\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3291250651769\t\t0.0019021163328345743\t\t0.4198352064265032\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3290933420376\t\t0.001736653105172514\t\t0.3825390869921606\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3290669265318\t\t0.0015855921334090297\t\t0.3485890725270157\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3290449292729\t\t0.0014476778530613957\t\t0.31768048224135786\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.3290266102144\t\t0.001321764535829626\t\t0.28953698783882686\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3290113535045\t\t0.0012068066132633897\t\t0.2639078925655109\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.328998646589\t\t0.0011018498646208936\t\t0.24056568243366375\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.3289880628445\t\t0.0010060233897583159\t\t0.21930382092080591\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289792471378\t\t0.000918532295465121\t\t0.19993476174046695\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289719038235\t\t0.0008386510305066106\t\t0.1822881569458446\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.3289657867576\t\t0.0007657173106354713\t\t0.1662092403307024\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.328960690997\t\t0.0006991265804704783\t\t0.1515573680763188\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.3289564458892\t\t0.000638326963929454\t\t0.13820470067107218\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.3289529093272\t\t0.0005828146595722881\t\t0.1260350118599105\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.328949962967\t\t0.0005321297409841108\t\t0.11494261177196871\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.3289475082504\t\t0.0004858523262016006\t\t0.10483137288139666\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.32894546309\t\t0.00044359908337082124\t\t0.09561384858180089\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.3289437591166\t\t0.00040502004275331746\t\t0.08721047519932054\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.3289423393812\t\t0.00036979568803279206\t\t0.0795488492594598\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.3289411564483\t\t0.00033763430219639775\t\t0.0725630726806353\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289401708032\t\t0.00030826954554681694\t\t0.06619315923909144\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289393495288\t\t0.00028145824538959704\t\t0.06038449644135023\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289386652032\t\t0.00025697837879067563\t\t0.055087357417150276\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.3289380949818\t\t0.00023462723140467258\t\t0.050256458056873655\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289376198327\t\t0.0002142197169572501\t\t0.0458505550912067\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.3289372238999\t\t0.00019558684329964385\t\t0.04183208118695903\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.3289368939722\t\t0.00017857431213231842\t\t0.03816681361237879\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.3289366190434\t\t0.0001630412408120689\t\t0.034823573247849154\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.328936389943\t\t0.000148858995443625\t\t0.03177395111997322\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.3289361990296\t\t0.00013591012562235184\t\t0.02899205989069037\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289360399372\t\t0.0001240873919154365\t\t0.026454307974490627\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.3289359073601\t\t0.00011329287797442755\t\t0.02413919412601071\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.3289357968793\t\t0.00010343717993045261\t\t0.022027120716959125\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.3289357048104\t\t9.443866630500864e-05\t\t0.020100223831181624\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289356280852\t\t8.622280228896286e-05\t\t0.0183422187557618\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.328935564146\t\t7.872153282149249e-05\t\t0.016738259372518095\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289355108618\t\t7.187271930134964e-05\t\t0.015274810205036597\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.3289354664566\t\t6.561962527150989e-05\t\t0.01393952996044609\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.3289354294507\t\t5.99104468295938e-05\t\t0.012721165549295532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.328935398611\t\t5.469788387839045e-05\t\t0.011609455543319496\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289353729103\t\t4.993874862120873e-05\t\t0.010595042320524987\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.3289353514917\t\t4.5593608112451605e-05\t\t0.009669392034702235\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289353336418\t\t4.162645786358954e-05\t\t0.008824721726902166\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.328935318766\t\t3.800442383434938e-05\t\t0.00805393295901731\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.3289353063687\t\t3.469749030711793e-05\t\t0.007350551310380624\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352960371\t\t3.1678251433566544e-05\t\t0.006708671308400825\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.3289352874267\t\t2.892168435036673e-05\t\t0.006122906214975856\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.3289352802508\t\t2.640494204908047e-05\t\t0.005588342282306295\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352742704\t\t2.4107164212426793e-05\t\t0.005100497068435964\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352692864\t\t2.2009304504146496e-05\t\t0.004655281451736754\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352651327\t\t2.009397287655695e-05\t\t0.0042489649923862696\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.3289352616712\t\t1.834529159570507e-05\t\t0.003878144367794669\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[116]\t\t1219.3289352587863\t\t1.6748763726013593e-05\t\t0.003539714620421952\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.328935256382\t\t1.529115311152935e-05\t\t0.003230842895431509\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352543782\t\t1.396037475461744e-05\t\t0.0029489445533669636\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.328935252708\t\t1.2745394709010706e-05\t\t0.002691661347754865\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.3289352513164\t\t1.1636138730630843e-05\t\t0.0024568415513909372\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.3289352501565\t\t1.0623408787906223e-05\t\t0.002242521829148132\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.3289352491897\t\t9.698806895741866e-06\t\t0.002046910702528696\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.328935248384\t\t8.854665551670342e-06\t\t0.001868373473457495\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352477127\t\t8.083984195457856e-06\t\t0.0017054184724173205\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.328935247153\t\t7.380371236315642e-06\t\t0.001556684526896393\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.3289352466868\t\t6.737991063822136e-06\t\t0.001420929519200739\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.328935246298\t\t6.1515157217817505e-06\t\t0.0012970199472343495\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.328935245974\t\t5.616080747571476e-06\t\t0.0011839214264864266\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.328935245704\t\t5.12724486795174e-06\t\t0.0010806899896295978\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352454792\t\t4.680953185822045e-06\t\t0.0009864641850432221\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.3289352452916\t\t4.2735035749662624e-06\t\t0.0009004578404134382\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352451352\t\t3.901516010211443e-06\t\t0.0008219534901043742\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.3289352450051\t\t3.5619045297194384e-06\t\t0.0007502963492647777\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.3289352448965\t\t3.251851664121548e-06\t\t0.0006848888338911066\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352448062\t\t2.968785095121938e-06\t\t0.0006251855589334022\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352447307\t\t2.710356320659649e-06\t\t0.0005706887760380466\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352446677\t\t2.4744211685005284e-06\t\t0.0005209441987584407\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.3289352446154\t\t2.259022051079068e-06\t\t0.0004755372112913061\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352445718\t\t2.0623717137413475e-06\t\t0.0004340894034650171\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352445354\t\t1.8828384264183577e-06\t\t0.0003962553963376544\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352445051\t\t1.7189324418977825e-06\t\t0.0003617199499544575\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.32893524448\t\t1.5692936572248397e-06\t\t0.0003301953582755339\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.3289352444588\t\t1.4326803169084676e-06\t\t0.00030141902126342877\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.3289352444413\t\t1.3079587423534921e-06\t\t0.0002751512446264095\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.3289352444267\t\t1.19409389522236e-06\t\t0.0002511732633408963\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.3289352444144\t\t1.090140822666348e-06\t\t0.00022928539624372156\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.3289352444042\t\t9.952368093957995e-07\t\t0.00020930538453102526\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.3289352443958\t\t9.08594209763928e-07\t\t0.00019106687254455984\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443887\t\t8.294939386393865e-07\t\t0.00017441801586670695\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.3289352443828\t\t7.572794936289108e-07\t\t0.00015922021203016628\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.3289352443778\t\t6.913514968201173e-07\t\t0.00014534695683631642\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.328935244374\t\t6.311627481399734e-07\t\t0.00013268277657798805\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443703\t\t5.762136713725847e-07\t\t0.00012112226326592354\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.3289352443676\t\t5.260481683139933e-07\t\t0.00011056920593915357\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.3289352443653\t\t4.802498427554657e-07\t\t0.00010093578506662147\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.3289352443635\t\t4.3843853194137825e-07\t\t9.214184504404175e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.3289352443614\t\t4.002671768929492e-07\t\t8.411420466929212e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.3289352443603\t\t3.654189091957985e-07\t\t7.678607566184684e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.3289352443592\t\t3.3360446021564067e-07\t\t7.009649166697962e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443583\t\t3.0455972811298693e-07\t\t6.398979551629189e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443574\t\t2.7804359781495755e-07\t\t5.841518905559062e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.3289352443567\t\t2.53835951050932e-07\t\t5.332629969068465e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443562\t\t2.3173582186409348e-07\t\t4.86807989665136e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.3289352443558\t\t2.1155974573194287e-07\t\t4.444004700910036e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.3289352443553\t\t1.9314021736515507e-07\t\t4.056877143914482e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.328935244355\t\t1.7632432416184443e-07\t\t3.703477651595149e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.3289352443546\t\t1.609724553027105e-07\t\t3.3808674394988244e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.3289352443544\t\t1.469571595791491e-07\t\t3.0863632139003e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.3289352443542\t\t1.3416207567086567e-07\t\t2.8175157402275337e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.3289352443544\t\t1.2248097717713901e-07\t\t2.5720899069008747e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.328935244354\t\t1.1181687746785354e-07\t\t2.348045076487767e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.328935244354\t\t1.0208123695518237e-07\t\t2.143517808392004e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.328935244354\t\t9.319323179252664e-08\t\t1.9568076510080335e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.328935244354\t\t8.507905962790291e-08\t\t1.7863626850495503e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443537\t\t7.76713476011212e-08\t\t1.6307655130354237e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.3289352443535\t\t7.090859425138442e-08\t\t1.4887229076488962e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443537\t\t6.473464541769311e-08\t\t1.3590530352655139e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t5.9098246646811706e-08\t\t1.240678835933295e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t5.395258470270543e-08\t\t1.1326162042021916e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443537\t\t4.925494432638312e-08\t\t1.0339662347572992e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t4.496631402382376e-08\t\t9.439094511287813e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t4.105108344651157e-08\t\t8.616969413134505e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t3.747674511712151e-08\t\t7.866457982717838e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t3.4213618677900964e-08\t\t7.181316176375145e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t3.123460817267446e-08\t\t6.555853095778472e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t2.851497567623946e-08\t\t5.984869196126349e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443535\t\t2.6032138208614063e-08\t\t5.463617024396255e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t2.3765481623559572e-08\t\t4.987765965802677e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443535\t\t2.1696181101338902e-08\t\t4.553362361220582e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.980705373732746e-08\t\t4.156798597060829e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443535\t\t1.808241326655956e-08\t\t3.7947697190352874e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[192]\t\t1219.3289352443535\t\t1.6507936626995305e-08\t\t3.4642716620206468e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[193]\t\t1219.3289352443535\t\t1.507055393284526e-08\t\t3.1625633714187983e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[194]\t\t1219.3289352443533\t\t1.375832007681099e-08\t\t2.8871280574141516e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[195]\t\t1219.3289352443535\t\t1.2560348800054677e-08\t\t2.6356852322952586e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[196]\t\t1219.3289352443535\t\t1.1466679896956103e-08\t\t2.40614418339409e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[197]\t\t1219.3289352443535\t\t1.0468245446797738e-08\t\t2.1965936870098344e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[198]\t\t1219.3289352443533\t\t9.556740967200614e-09\t\t2.0052914800831207e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.010097285713812318s\n",
      "Estimated average function evaluation times: 0.00034183323985398416\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1282.8619962630191\t\t2.073169807606151\t\t35977.483420369594\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1255.8306337221459\t\t1.601129330258352\t\t688.0262423775044\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1242.397285236013\t\t1.2666280518843254\t\t471.80328577704586\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1234.4996667414339\t\t1.0048629094264765\t\t351.5254198258405\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1229.5832453603705\t\t0.7993355440053378\t\t271.3798778442975\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1226.4290845262424\t\t0.6388223272462425\t\t213.32963143146048\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1224.3569975335154\t\t0.5137788879325308\t\t169.66203459226227\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1222.9653541548114\t\t0.41631940588831134\t\t136.1576667860605\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1222.010420465975\t\t0.34016888307816046\t\t110.13307678256943\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1221.3413339112285\t\t0.2804410378863169\t\t89.74039316388821\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1220.8630344346825\t\t0.23336888340997225\t\t73.65031847368965\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1220.5145720950284\t\t0.19605861949682804\t\t60.880479712952074\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1220.256183912296\t\t0.16628846496571165\t\t50.69141373084108\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1220.0614738872985\t\t0.14235203536965704\t\t42.51907613462807\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1219.9126104503725\t\t0.12293869133922648\t\t35.92904649191041\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1219.7973344563875\t\t0.10704238700882966\t\t30.584428816556485\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1219.7070689351108\t\t0.09389184310032418\t\t26.22275590637909\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1219.635708393479\t\t0.08289661957116481\t\t22.638989994234795\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1219.578832727711\t\t0.07360520149738739\t\t19.67275188835053\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1219.5331897871417\t\t0.06567234947614817\t\t17.19855249336104\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.4963499999892\t\t0.05883372550814732\t\t15.118211129677615\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.4664725357125\t\t0.05288628852743883\t\t13.354911107648412\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.4421446509455\t\t0.04767326680122571\t\t11.848514326676924\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[24]\t\t1219.4222696512977\t\t0.04307273574049999\t\t10.551865365831533\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.405987562955\t\t0.03898900451054457\t\t9.427884064823967\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.3926181049474\t\t0.035346163651791586\t\t8.447289768849442\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.381619075161\t\t0.032083275341716545\t\t7.586830715419108\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.3725555415144\t\t0.029150799039990103\t\t6.827914774978726\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.36507671744\t\t0.02650793813506359\t\t6.155556244003645\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.358898381823\t\t0.024120668621348283\t\t5.5575691292146585\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.3537893569285\t\t0.02196027046768085\t\t5.02395091462716\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.349560997735\t\t0.020002228451114327\t\t4.546412354122891\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3460589455676\t\t0.01822540425167181\t\t4.118018484557163\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.343156605265\t\t0.01661140782545161\t\t3.732913944794572\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.340749949052\t\t0.015144115492123151\t\t3.386112002538031\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.3387533520572\t\t0.01380929643494048\t\t3.073331656695335\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.337096237255\t\t0.012594319723438654\t\t2.7908710266486616\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.3357203605308\t\t0.011487921537901935\t\t2.535508179236323\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.3345776054468\t\t0.010480017762489404\t\t2.3044227687920906\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.3336281862914\t\t0.00956155108701395\t\t2.0951335377291245\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3328391798238\t\t0.008724364633193732\t\t1.9054479740257708\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.3321833228079\t\t0.007961096204245477\t\t1.733421351737433\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.331638025265\t\t0.007265088767851765\t\t1.5773230711819164\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.3311845593632\t\t0.006630313882009091\t\t1.4356087276478275\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.330807391705\t\t0.0060513055754453085\t\t1.3068967177236346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.330493632955\t\t0.005523102782494639\t\t1.1899484750241136\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3302325836892\t\t0.005041198865678922\t\t1.0836516376323453\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.3300153592772\t\t0.004601497080616739\t\t0.9870056072489408\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.329834579779\t\t0.004200271077837348\t\t0.8991090782081883\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.3296841133943\t\t0.003834129716782179\t\t0.8191492042307174\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3295588640763\t\t0.003499985604552164\t\t0.746392138415165\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.3294545955996\t\t0.0031950268773552376\t\t0.6801747344277537\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.3293677857484\t\t0.002916691824296622\t\t0.6198972365971305\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.32929550541\t\t0.0026626460173368584\t\t0.5650168182592804\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.329235318276\t\t0.002430761662150283\t\t0.5150418519667661\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3291851976078\t\t0.002219098925566183\t\t0.4695268146991046\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.3291434571352\t\t0.0020258890284995386\t\t0.42806774679056964\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.3291086936752\t\t0.001849518920764046\t\t0.3902981957095658\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.3290797394625\t\t0.001688517376882189\t\t0.3558855861326331\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3290556225438\t\t0.0015415423711534258\t\t0.3245279660477853\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.32903553386\t\t0.0014073696066107992\t\t0.29595108558247957\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.329018799881\t\t0.0012848820863499409\t\t0.26990577105648333\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.3290048598542\t\t0.0011730606277341722\t\t0.24616556159815892\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3289932468836\t\t0.0010709752305945654\t\t0.22452457975326592\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3289835721903\t\t0.0009777772194663004\t\t0.20479561108426095\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3289755120213\t\t0.0008926920881918083\t\t0.1868083706962269\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3289687967574\t\t0.0008150129821667672\t\t0.17040793725299908\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.3289632018464\t\t0.0007440947599189091\t\t0.15545333726516578\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3289585402626\t\t0.0006793485813357845\t\t0.14181626433754094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.3289546562262\t\t0.0006202369747818398\t\t0.12937991985279632\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.3289514199782\t\t0.0005662693399957996\t\t0.11803796292650459\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289487234288\t\t0.0005169978476227166\t\t0.10769355890926217\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289464765355\t\t0.00047201369980664126\t\t0.09825851668440062\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.3289446042877\t\t0.00043094371970596294\t\t0.08965250620493932\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.3289430441941\t\t0.0003934472405613043\t\t0.08180234848368073\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.328941744193\t\t0.0003592132678288482\t\t0.07464137107032154\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.3289406609088\t\t0.00032795789001736545\t\t0.06810882279849699\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.3289397582034\t\t0.0002994219164304541\t\t0.062149342143653634\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.3289390059674\t\t0.00027336872158336664\t\t0.05671247418927158\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.3289383791125\t\t0.0002495822782225107\t\t0.05175223155264633\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.3289378567365\t\t0.00022786536225808535\t\t0.04722669522068995\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.3289374214214\t\t0.00020803791454375713\t\t0.0430976515961313\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.328937058655\t\t0.0001899355456823079\t\t0.03933026233237538\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289367563439\t\t0.0001734081713426494\t\t0.03589276395450746\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289365044118\t\t0.0001583187666540676\t\t0.03275619456858381\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289362944615\t\t0.00014454222922677664\t\t0.029894145099095368\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.3289361194966\t\t0.0001319643412651638\t\t0.027282532871667358\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289359736864\t\t0.00012048082219080703\t\t0.024899395455404375\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.328935852172\t\t0.00010999646379373655\t\t0.022724702967680203\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.328935750905\t\t0.00010042434072015985\t\t0.02074018711856204\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.328935666511\t\t9.168508972468614e-05\t\t0.018929185518270934\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.3289355961779\t\t8.370625170136601e-05\t\t0.01727649985229404\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.3289355375637\t\t7.642167096476471e-05\t\t0.015768266662264427\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289354887152\t\t6.977094688258465e-05\t\t0.014391839613254182\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.3289354480053\t\t6.369893316597807e-05\t\t0.013135682220280779\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.328935414078\t\t5.815528079984594e-05\t\t0.011989270092882125\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.3289353858027\t\t5.309402071081461e-05\t\t0.010943001810891537\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289353622383\t\t4.847318276576564e-05\t\t0.009988117728332458\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.3289353425996\t\t4.4254447902934265e-05\t\t0.009116625927266935\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289353262328\t\t4.040283056231231e-05\t\t0.008321234734779467\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.3289353125926\t\t3.688638868443509e-05\t\t0.007595291198134533\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.3289353012249\t\t3.367595901152154e-05\t\t0.006932724971935315\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.328935291751\t\t3.074491534441517e-05\t\t0.006327997178797895\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289352838551\t\t2.8068947875549386e-05\t\t0.005776053762675134\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.3289352772747\t\t2.562586169055811e-05\t\t0.00527228295156859\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289352717907\t\t2.3395392811945943e-05\t\t0.00481247647164986\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.32893526722\t\t2.135904021912866e-05\t\t0.004392794163932188\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.3289352634108\t\t1.9499912514425165e-05\t\t0.004009731731769967\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352602362\t\t1.7802587899798206e-05\t\t0.003660091297339719\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.3289352575903\t\t1.6252986329314e-05\t\t0.0033409545685782324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.3289352553852\t\t1.483825280600509e-05\t\t0.0030496583577876053\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352535478\t\t1.354665080425365e-05\t\t0.002783772217903464\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352520162\t\t1.2367464981507615e-05\t\t0.0025410781182077314\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352507397\t\t1.1290912322673256e-05\t\t0.0023195518051308747\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.3289352496759\t\t1.030806106229808e-05\t\t0.0021173458915735386\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[116]\t\t1219.328935248789\t\t9.4107566015529e-06\t\t0.0019327743325118012\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.3289352480504\t\t8.591553910921108e-06\t\t0.0017642983781816094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352474344\t\t7.843655795712154e-06\t\t0.0016105136252878415\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.3289352469212\t\t7.160856537289731e-06\t\t0.0014701383294143248\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.3289352464935\t\t6.537490424939655e-06\t\t0.0013420026433340656\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.328935246137\t\t5.968384779208277e-06\t\t0.0012250388538745257\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.3289352458398\t\t5.448817058527748e-06\t\t0.001118272450247293\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.3289352455922\t\t4.974475688007344e-06\t\t0.0010208139900664429\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352453858\t\t4.541424296169314e-06\t\t0.0009318516464998232\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.3289352452139\t\t4.146069094006997e-06\t\t0.000850644458030665\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.3289352450706\t\t3.7851290315834e-06\t\t0.0007765161066849994\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.328935244951\t\t3.4556086086479703e-06\t\t0.0007088492989357029\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.3289352448514\t\t3.154773032469105e-06\t\t0.0006470805874766876\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.3289352447687\t\t2.8801255013093998e-06\t\t0.000590695696170315\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352446996\t\t2.6293865283823007e-06\t\t0.0005392252081212484\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.3289352446418\t\t2.400475015661454e-06\t\t0.000492240646790147\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352445938\t\t2.191490994053028e-06\t\t0.00044935091617315226\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.3289352445538\t\t2.000699846811885e-06\t\t0.00041019904015696037\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.3289352445204\t\t1.8265179470911045e-06\t\t0.0003744591615612663\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352444924\t\t1.6674995028195588e-06\t\t0.0003418338492869567\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352444694\t\t1.5223245517971334e-06\t\t0.00031205161364951915\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352444501\t\t1.3897880269835878e-06\t\t0.00028486462994231794\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.328935244434\t\t1.2687897540561849e-06\t\t0.0002600466908174964\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352444203\t\t1.1583253187070224e-06\t\t0.00023739130641452395\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352444094\t\t1.0574777268044892e-06\t\t0.000216710003271097\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352444\t\t9.65409813261642e-07\t\t0.00019783072292329094\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.3289352443921\t\t8.813572737093149e-07\t\t0.00018059641637121167\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.3289352443858\t\t8.046223463522573e-07\t\t0.00016486372898616758\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.3289352443803\t\t7.345679986647127e-07\t\t0.0001505017925739497\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.328935244376\t\t6.70612647735043e-07\t\t0.00013739115638747838\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.3289352443721\t\t6.122253415050802e-07\t\t0.00012542278166874886\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.328935244369\t\t5.589213446074598e-07\t\t0.0001144971246275428\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.3289352443664\t\t5.102581241279827e-07\t\t0.00010452332894202707\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443644\t\t4.658316571251667e-07\t\t9.541845724576354e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.3289352443624\t\t4.252731046746318e-07\t\t8.710678521889997e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.3289352443608\t\t3.8824572791276605e-07\t\t7.951921014723782e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.3289352443596\t\t3.54442105820011e-07\t\t7.259263466192924e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443585\t\t3.2358157925417643e-07\t\t6.626946533150134e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.3289352443578\t\t2.954079209466165e-07\t\t6.0497130094498224e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.3289352443571\t\t2.6968720585195924e-07\t\t5.5227641322145575e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.3289352443562\t\t2.4620588490379604e-07\t\t5.0417184042777846e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.328935244356\t\t2.247689848136036e-07\t\t4.602577024881312e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.3289352443555\t\t2.0519851946287017e-07\t\t4.201688867997482e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.328935244355\t\t1.873319867641701e-07\t\t3.835721361378341e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443549\t\t1.7102104070652865e-07\t\t3.5016323063125817e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443546\t\t1.5613024530466968e-07\t\t3.196644683587354e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.3289352443544\t\t1.4253595066794282e-07\t\t2.9182235438470538e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443544\t\t1.3012528402925774e-07\t\t2.6640537203210676e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.3289352443542\t\t1.1879519287152388e-07\t\t2.432022992237674e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.3289352443542\t\t1.084515929746786e-07\t\t2.220203135045355e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.328935244354\t\t9.900860034064648e-08\t\t2.0268333228667468e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.3289352443537\t\t9.038779776379054e-08\t\t1.8503058489635375e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.328935244354\t\t8.251760384348023e-08\t\t1.6891544796846414e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.3289352443537\t\t7.5332663455474e-08\t\t1.5420393663726306e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.3289352443537\t\t6.877331140081728e-08\t\t1.4077376476234018e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.3289352443537\t\t6.278508528110954e-08\t\t1.2851337478294268e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.3289352443537\t\t5.731825235523366e-08\t\t1.1732082060135209e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.3289352443535\t\t5.232741863876793e-08\t\t1.0710311392589391e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.3289352443535\t\t4.7771141851277754e-08\t\t9.777532275147027e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443537\t\t4.361158024757522e-08\t\t8.925996486630135e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.3289352443535\t\t3.9814200888333763e-08\t\t8.148624789795718e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443535\t\t3.634746121019763e-08\t\t7.438956636740666e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t3.3182579579217344e-08\t\t6.7910966937406285e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t3.0293265416519155e-08\t\t6.199662693773829e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443535\t\t2.765553006644595e-08\t\t5.659739188197934e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t2.52474656126288e-08\t\t5.1668394630844415e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t2.304908043006609e-08\t\t4.716865132077922e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t2.104211200237958e-08\t\t4.306082027959413e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t1.920989621627775e-08\t\t3.931078090633689e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t1.7537212615990978e-08\t\t3.5887309876324354e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t1.601017756994602e-08\t\t3.2761974123252907e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443533\t\t1.4616103238509924e-08\t\t2.9908802602516048e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t1.3343415665535809e-08\t\t2.7304162020996605e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443535\t\t1.2181547788331465e-08\t\t2.4926315927460377e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.1120846821187642e-08\t\t2.2755597794629448e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443535\t\t1.0152504844775289e-08\t\t2.0773881448982975e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[192]\t\t1219.3289352443535\t\t9.268477820773108e-09\t\t1.8964795782508634e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.010237182180086771s\n",
      "Estimated average function evaluation times: 0.00033885737260182697\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[1]\t\t1285.8668282714434\t\t1.9364179274731161\t\t33981.086375705556\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[2]\t\t1261.5798648119571\t\t1.352724515830308\t\t567.7305420911274\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[3]\t\t1249.0062920355504\t\t1.0176315358528085\t\t379.26849430149446\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[4]\t\t1241.09216982186\t\t0.8027296052750369\t\t286.07921744371436\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[5]\t\t1235.6581613906687\t\t0.6537245390836679\t\t227.22839931401603\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[6]\t\t1231.7742634071733\t\t0.545179049478061\t\t184.49363508581766\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[7]\t\t1228.9305677565005\t\t0.4632117389830468\t\t151.81541589595335\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[8]\t\t1226.8109041683856\t\t0.39940826533714324\t\t126.3230100932022\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[9]\t\t1225.2078052094541\t\t0.3483841468476209\t\t106.18987193671737\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[10]\t\t1223.9805748273493\t\t0.3065969015448716\t\t90.10835341654958\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[11]\t\t1223.0314160316875\t\t0.2716721001476041\t\t77.11463766265965\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[12]\t\t1222.2909184564073\t\t0.2419894653118186\t\t66.49484291060871\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[13]\t\t1221.708913266707\t\t0.21642047983321538\t\t57.71906577099855\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[14]\t\t1221.2485562268807\t\t0.1941603163067872\t\t50.392173020761376\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[15]\t\t1220.882409893236\t\t0.17461981442311536\t\t44.21732774067202\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[16]\t\t1220.589793464801\t\t0.1573558335890703\t\t38.969370574613656\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[17]\t\t1220.3549542674714\t\t0.1420260915591319\t\t34.475609009569\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[18]\t\t1220.1657830455756\t\t0.1283596057377985\t\t30.602024805258218\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[19]\t\t1220.012896404366\t\t0.1161370880883513\t\t27.24337185634234\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[20]\t\t1219.8889717823404\t\t0.10517771834627712\t\t24.3160388754049\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[21]\t\t1219.788259110606\t\t0.09533003744673692\t\t21.752869137881788\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[22]\t\t1219.7062180297125\t\t0.08646553348697772\t\t19.49936663793834\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[23]\t\t1219.6392455875553\t\t0.07847401369904507\t\t17.510888731157706\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[24]\t\t1219.5844699644674\t\t0.07126018251863614\t\t15.750545666789757\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[25]\t\t1219.5395929251326\t\t0.06474105085241919\t\t14.187611191632877\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[26]\t\t1219.5027685941918\t\t0.05884393091001747\t\t12.796306409323746\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[27]\t\t1219.4725095561218\t\t0.053504853055405636\t\t11.55485919647917\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[28]\t\t1219.4476136786318\t\t0.04866729377584073\t\t10.444769298325804\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[29]\t\t1219.427106770625\t\t0.04428113804504521\t\t9.45022862467217\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[30]\t\t1219.410197421389\t\t0.040301821875387535\t\t8.557659892750596\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[31]\t\t1219.3962412688923\t\t0.03668961593907752\t\t7.755346419908337\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[32]\t\t1219.3847126085616\t\t0.033409021431483964\t\t7.033132780576505\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[33]\t\t1219.3751817466587\t\t0.03042825651078223\t\t6.382181040154685\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[34]\t\t1219.3672968711833\t\t0.02771881673020256\t\t5.794770931629898\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[35]\t\t1219.360769491307\t\t0.025255096558088892\t\t5.264135038801943\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[36]\t\t1219.3553627074316\t\t0.023014061795283827\t\t4.784322063081106\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[37]\t\t1219.3508817351972\t\t0.020974964737931995\t\t4.350082767309084\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[38]\t\t1219.3471662306692\t\t0.019119095490664453\t\t3.956774342845233\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[39]\t\t1219.344084059639\t\t0.017429564041290483\t\t3.6002798302424526\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[40]\t\t1219.3415262283086\t\t0.015891108655627907\t\t3.2769399067697567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[41]\t\t1219.3394027506308\t\t0.014489926903678611\t\t2.983494885908985\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[42]\t\t1219.3376392730488\t\t0.013213526232551863\t\t2.717035190825084\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[43]\t\t1219.3361743131632\t\t0.012050591491076413\t\t2.474958892419843\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[44]\t\t1219.334956997156\t\t0.010990867210617882\t\t2.2549351635452273\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[45]\t\t1219.3339452032467\t\t0.01002505277549926\t\t2.0548727090873387\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[46]\t\t1219.3331040363148\t\t0.009144708888479786\t\t1.872892398386586\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[47]\t\t1219.3324045731206\t\t0.008342173963467907\t\t1.7073034611315212\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[48]\t\t1219.331822828979\t\t0.007610489267283244\t\t1.5565827165364627\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[49]\t\t1219.331338905947\t\t0.006943331791987505\t\t1.4193563943775478\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[50]\t\t1219.3309362899852\t\t0.006334953974139191\t\t1.2943841785604127\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[51]\t\t1219.3306012705548\t\t0.005780129491878354\t\t1.1805451632645976\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[52]\t\t1219.3303224609422\t\t0.005274104468280553\t\t1.0768254605756535\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[53]\t\t1219.3300904015503\t\t0.004812553493020813\t\t0.9823072386046813\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[54]\t\t1219.3298972315943\t\t0.004391539945953623\t\t0.8961590028150113\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[55]\t\t1219.3297364172417\t\t0.004007480168026256\t\t0.8176269607663849\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[56]\t\t1219.3296025263703\t\t0.003657111078370707\t\t0.7460273340937463\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[57]\t\t1219.3294910418563\t\t0.0033374608826860176\t\t0.6807395005725814\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[58]\t\t1219.3293982067325\t\t0.0030458225582532705\t\t0.6211998659285665\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[59]\t\t1219.32932089572\t\t0.0027797298361872357\t\t0.5668963784466328\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[60]\t\t1219.3292565086022\t\t0.002536935432067284\t\t0.5173636114770004\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[61]\t\t1219.3292028816952\t\t0.002315391303202026\t\t0.4721783484989809\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[62]\t\t1219.329158214322\t\t0.002113230734423591\t\t0.43095561422262585\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[63]\t\t1219.329121007726\t\t0.0019287520751139736\t\t0.3933451020192262\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[64]\t\t1219.3290900143088\t\t0.0017604039687435548\t\t0.3590279545239321\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[65]\t\t1219.3290641954363\t\t0.0016067719323909183\t\t0.32771385930014424\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[66]\t\t1219.3290426863566\t\t0.0014665661583256467\t\t0.29913842611556174\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[67]\t\t1219.3290247670298\t\t0.0013386104225091952\t\t0.2730608164400199\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[68]\t\t1219.329009837863\t\t0.001221831996324559\t\t0.24926159894960873\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[69]\t\t1219.3289973995275\t\t0.0011152524682352812\t\t0.22754080811437302\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[70]\t\t1219.3289870361648\t\t0.0010179793908980026\t\t0.2077161852714521\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[71]\t\t1219.328978401412\t\t0.0009291986777627644\t\t0.18962158401953902\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[72]\t\t1219.3289712067747\t\t0.0008481676802798049\t\t0.17310552366796048\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[73]\t\t1219.3289652119438\t\t0.0007742088834513244\t\t0.15802987632712584\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[74]\t\t1219.32896021674\t\t0.0007067041635043221\t\t0.14426867456155132\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[75]\t\t1219.3289560544047\t\t0.0006450895565301427\t\t0.13170702815053723\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[76]\t\t1219.328952586016\t\t0.0005888504919528596\t\t0.12024013944423537\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[77]\t\t1219.3289496958357\t\t0.0005375174487887241\t\t0.10977240806842865\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[78]\t\t1219.3289472874405\t\t0.000490661996783836\t\t0.1002166164776052\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[79]\t\t1219.328945280493\t\t0.0004478931877470726\t\t0.09149318888958226\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[80]\t\t1219.3289436080588\t\t0.0004088542657913948\t\t0.08352951680269911\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[81]\t\t1219.3289422143675\t\t0.000373219668001702\t\t0.0762593447872891\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[82]\t\t1219.32894105295\t\t0.00034069228956663827\t\t0.069622211240853\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[83]\t\t1219.3289400850877\t\t0.0003110009898717068\t\t0.06356293888436093\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[84]\t\t1219.3289392785168\t\t0.0002838983181669663\t\t0.058031170586626775\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[85]\t\t1219.3289386063539\t\t0.00025915843920941407\t\t0.052980946362693855\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[86]\t\t1219.3289380461974\t\t0.00023657524132044436\t\t0.04837031781418189\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[87]\t\t1219.3289375793802\t\t0.00021596061057009875\t\t0.04416099666455993\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[88]\t\t1219.3289371903468\t\t0.00019714285649760703\t\t0.04031803432663447\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[89]\t\t1219.3289368661347\t\t0.00017996527593930662\t\t0.03680952970181304\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[90]\t\t1219.3289365959422\t\t0.0001642848427956667\t\t0.033606362694277726\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[91]\t\t1219.3289363707675\t\t0.00014997101264516222\t\t0.030681951193869236\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[92]\t\t1219.3289361831094\t\t0.00013690463208361554\t\t0.028012029361624207\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[93]\t\t1219.3289360267163\t\t0.0001249769435818919\t\t0.025574445398049872\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[94]\t\t1219.3289358963796\t\t0.00011408867745422772\t\t0.023348977024322905\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[95]\t\t1219.328935787757\t\t0.0001041492232928267\t\t0.021317163160633527\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[96]\t\t1219.328935697231\t\t9.50758738947217e-05\t\t0.01946215031086441\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[97]\t\t1219.3289356217863\t\t8.67931352963163e-05\t\t0.01776855238513028\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[98]\t\t1219.3289355589104\t\t7.923209715647769e-05\t\t0.016222322800506506\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[99]\t\t1219.3289355065094\t\t7.232985816329332e-05\t\t0.01481063773053433\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[100]\t\t1219.3289354628378\t\t6.602900164127768e-05\t\t0.013521789596258632\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[101]\t\t1219.3289354264418\t\t6.027711702284408e-05\t\t0.012345089799774486\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[102]\t\t1219.3289353961088\t\t5.502636305408019e-05\t\t0.011270779986346208\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[103]\t\t1219.328935370829\t\t5.023306920387404e-05\t\t0.010289951028916378\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[104]\t\t1219.3289353497605\t\t4.585737186215783e-05\t\t0.009394469098018332\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[105]\t\t1219.3289353322016\t\t4.186288229140853e-05\t\t0.008576908145172807\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[106]\t\t1219.3289353175676\t\t3.821638357918158e-05\t\t0.007830488308892568\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[107]\t\t1219.3289353053715\t\t3.488755402964656e-05\t\t0.0071490196509860675\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[108]\t\t1219.3289352952072\t\t3.184871471041174e-05\t\t0.006526850826711588\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[109]\t\t1219.3289352867362\t\t2.9074599002084282e-05\t\t0.005958822187606108\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[110]\t\t1219.328935279676\t\t2.654214229806633e-05\t\t0.005440223032613793\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[111]\t\t1219.328935273792\t\t2.42302900040472e-05\t\t0.0049667525060316025\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[112]\t\t1219.3289352688882\t\t2.2119822318270832e-05\t\t0.004534483943615536\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[113]\t\t1219.3289352648012\t\t2.0193194288969005e-05\t\t0.004139832319335639\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[114]\t\t1219.3289352613951\t\t1.8434389810963632e-05\t\t0.0037795245217088005\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[115]\t\t1219.3289352585564\t\t1.6828788373087243e-05\t\t0.003450572205568163\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116]\t\t1219.3289352561906\t\t1.5363043399297826e-05\t\t0.00315024703160437\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[117]\t\t1219.3289352542188\t\t1.4024971240975623e-05\t\t0.002876058025455706\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[118]\t\t1219.3289352525753\t\t1.2803449798259612e-05\t\t0.0026257309474927966\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[119]\t\t1219.3289352512058\t\t1.1688326029373597e-05\t\t0.0023971894180102303\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[120]\t\t1219.3289352500644\t\t1.067033146437489e-05\t\t0.002188537729226907\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[121]\t\t1219.328935249113\t\t9.741005161319575e-06\t\t0.0019980450909831316\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[122]\t\t1219.3289352483202\t\t8.892623297381731e-06\t\t0.0018241313141284766\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[123]\t\t1219.3289352476595\t\t8.118134973609506e-06\t\t0.0016653537059943817\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[124]\t\t1219.3289352471088\t\t7.411103537628567e-06\t\t0.001520395085396378\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[125]\t\t1219.32893524665\t\t6.765653099684246e-06\t\t0.001388052889154195\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[126]\t\t1219.328935246267\t\t6.176419647229964e-06\t\t0.0012672291843822065\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[127]\t\t1219.3289352459485\t\t5.638506452097914e-06\t\t0.0011569215782153543\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[128]\t\t1219.3289352456827\t\t5.147443356331233e-06\t\t0.0010562148882445588\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[129]\t\t1219.3289352454613\t\t4.699149613405413e-06\t\t0.0009642735798468324\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[130]\t\t1219.3289352452766\t\t4.289899946759167e-06\t\t0.0008803348125547745\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[131]\t\t1219.3289352451231\t\t3.916293590069543e-06\t\t0.0008037021232376706\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[132]\t\t1219.3289352449947\t\t3.575226017407562e-06\t\t0.0007337396648525479\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[133]\t\t1219.3289352448878\t\t3.263863125442637e-06\t\t0.0006698669115432629\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[134]\t\t1219.328935244799\t\t2.9796176804393623e-06\t\t0.0006115538620625019\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[135]\t\t1219.3289352447246\t\t2.720127818258379e-06\t\t0.000558316626766787\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[136]\t\t1219.3289352446627\t\t2.483237392120633e-06\t\t0.0005097134364277563\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[137]\t\t1219.3289352446113\t\t2.266978073914314e-06\t\t0.00046534096124020957\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[138]\t\t1219.3289352445681\t\t2.069552979968723e-06\t\t0.00042483096999250305\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[139]\t\t1219.3289352445324\t\t1.8893217350885086e-06\t\t0.0003878472819399537\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[140]\t\t1219.3289352445026\t\t1.7247868471445957e-06\t\t0.0003540829669547727\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[141]\t\t1219.3289352444779\t\t1.5745812504692475e-06\t\t0.0003232578131090765\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[142]\t\t1219.3289352444572\t\t1.4374569609435379e-06\t\t0.00029511598681131347\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[143]\t\t1219.32893524444\t\t1.3122746842532146e-06\t\t0.00026942393426131737\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[144]\t\t1219.3289352444253\t\t1.1979943576619029e-06\t\t0.000245968408221975\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[145]\t\t1219.3289352444135\t\t1.0936665054213783e-06\t\t0.00022455474384792726\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[146]\t\t1219.3289352444035\t\t9.984243504373577e-07\t\t0.0002050052016106331\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[147]\t\t1219.3289352443953\t\t9.114766078024406e-07\t\t0.0001871575153091639\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[148]\t\t1219.328935244388\t\t8.321009102784984e-07\t\t0.00017086354863783446\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[149]\t\t1219.3289352443824\t\t7.596378070671091e-07\t\t0.00015598805022748902\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[150]\t\t1219.3289352443776\t\t6.934852750580198e-07\t\t0.00014240755331063014\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[151]\t\t1219.3289352443735\t\t6.330937346608668e-07\t\t0.00013000931736445063\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[152]\t\t1219.3289352443703\t\t5.779614600383214e-07\t\t0.0001186904358805966\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[153]\t\t1219.3289352443674\t\t5.276304297748007e-07\t\t0.0001083569443865532\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[154]\t\t1219.328935244365\t\t4.816825099037461e-07\t\t9.89230688770382e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[155]\t\t1219.328935244363\t\t4.3973598506590016e-07\t\t9.031049116348764e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[156]\t\t1219.3289352443617\t\t4.014423776394258e-07\t\t8.244771486459912e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[157]\t\t1219.3289352443603\t\t3.664835709016326e-07\t\t7.526947063927607e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[158]\t\t1219.328935244359\t\t3.345691419521221e-07\t\t6.871616280453501e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[159]\t\t1219.328935244358\t\t3.0543396037494667e-07\t\t6.273339208507115e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[160]\t\t1219.3289352443574\t\t2.7883600027955303e-07\t\t5.727148991017053e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[161]\t\t1219.3289352443567\t\t2.545542953328503e-07\t\t5.228510841274877e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[162]\t\t1219.3289352443562\t\t2.323871363116515e-07\t\t4.773284869132085e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[163]\t\t1219.3289352443558\t\t2.121503796303288e-07\t\t4.3576922564797523e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[164]\t\t1219.3289352443553\t\t1.936759092585598e-07\t\t3.978282509738582e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[165]\t\t1219.3289352443549\t\t1.7681025409867884e-07\t\t3.6319050546652634e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[166]\t\t1219.3289352443549\t\t1.6141331603626166e-07\t\t3.315684693192582e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[167]\t\t1219.3289352443544\t\t1.4735718924445936e-07\t\t3.026995796936843e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[168]\t\t1219.3289352443544\t\t1.3452510923174736e-07\t\t2.7634415472826773e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[169]\t\t1219.3289352443542\t\t1.228104827903692e-07\t\t2.5228337857515578e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[170]\t\t1219.3289352443542\t\t1.1211599238146333e-07\t\t2.3031744873386238e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[171]\t\t1219.328935244354\t\t1.0235280740677048e-07\t\t2.1026398802111467e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[172]\t\t1219.328935244354\t\t9.343981954325318e-08\t\t1.919565068921297e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[173]\t\t1219.3289352443537\t\t8.530299385107366e-08\t\t1.7524297711748978e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[174]\t\t1219.328935244354\t\t7.787473802970444e-08\t\t1.5998468709780235e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[175]\t\t1219.3289352443537\t\t7.109335002614797e-08\t\t1.4605484562996893e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[176]\t\t1219.3289352443537\t\t6.490249263709003e-08\t\t1.3333783737490246e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[177]\t\t1219.3289352443535\t\t5.925075301636301e-08\t\t1.2172810862834178e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[178]\t\t1219.3289352443535\t\t5.409116867163601e-08\t\t1.1112914669800095e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[179]\t\t1219.3289352443535\t\t4.9380891639599064e-08\t\t1.0145306287738074e-05\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[180]\t\t1219.3289352443535\t\t4.50807914875432e-08\t\t9.261947871584182e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[181]\t\t1219.3289352443535\t\t4.115514380193161e-08\t\t8.45550136639349e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[182]\t\t1219.3289352443535\t\t3.7571351632686074e-08\t\t7.719269117118718e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[183]\t\t1219.3289352443535\t\t3.429963403312259e-08\t\t7.047142705441034e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[184]\t\t1219.3289352443535\t\t3.1312826220686e-08\t\t6.4335356693173875e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[185]\t\t1219.3289352443535\t\t2.858610432016488e-08\t\t5.873355542451726e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[186]\t\t1219.3289352443535\t\t2.6096831585771032e-08\t\t5.361948940811507e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[187]\t\t1219.3289352443535\t\t2.3824328122097587e-08\t\t4.895074067311164e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[188]\t\t1219.3289352443535\t\t2.1749710518438076e-08\t\t4.468846811444575e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[189]\t\t1219.3289352443535\t\t1.9855750467068862e-08\t\t4.079737515747857e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[190]\t\t1219.3289352443535\t\t1.812672040803659e-08\t\t3.724502513273169e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[191]\t\t1219.3289352443533\t\t1.6548255088700674e-08\t\t3.4002008630656306e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[192]\t\t1219.3289352443535\t\t1.510724152349086e-08\t\t3.1041336601692594e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[193]\t\t1219.3289352443535\t\t1.3791712379725823e-08\t\t2.8338467770177195e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[194]\t\t1219.3289352443535\t\t1.2590740171756947e-08\t\t2.5870972778278336e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[195]\t\t1219.3289352443535\t\t1.1494344410356089e-08\t\t2.361829120989804e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[196]\t\t1219.3289352443535\t\t1.0493426603922519e-08\t\t2.156174873655158e-06\n",
      "\n",
      "Iteration\tFunction Eval (Loss)\t\tCurrent ||grad_u||\t\tPrev ||grad_v||\t\n",
      "[197]\t\t1219.3289352443535\t\t9.579667574940273e-09\t\t1.968432692714121e-06\n",
      "--------------------------------------------------------------------------------\n",
      "Estimated average iteration runtime: 0.010700371059669455s\n",
      "Estimated average function evaluation times: 0.0003711451128654674\n",
      "Train Mean Squared error is: 1.0705258430591338\n"
     ]
    }
   ],
   "source": [
    "from als import ALSSparse\n",
    "from main import average_stats, init_vector\n",
    "\n",
    "def run_experiment(data: MovieLensDataset,\n",
    "                   sparse=True,\n",
    "                   grad_sensibility=1e-8,\n",
    "                   num_experiments=1,\n",
    "                   warmup=0,\n",
    "                   workers=8):\n",
    "#     date = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    trainX = data.X\n",
    "\n",
    "    # print(trainX.shape, testX.shape)\n",
    "    # optional warmup\n",
    "    for _ in range(warmup):\n",
    "        u = init_vector(data.n_users, normalize=True)\n",
    "        v = init_vector(data.n_movies, normalize=True)\n",
    "        args = [u, v, trainX]\n",
    "        als = ALSSparse(*args) if sparse else ALS(*args)\n",
    "        u, v = als.fit(eps_g=grad_sensibility)\n",
    "\n",
    "    stats = {}\n",
    "    start = time.time()\n",
    "    for i in range(num_experiments):\n",
    "        u = init_vector(data.n_users, normalize=True)\n",
    "        v = init_vector(data.n_movies, normalize=True)\n",
    "        args = [u, v, trainX]\n",
    "        als = ALSSparse(*args) if sparse else ALS(*args)\n",
    "        # run Alternating Least Squares algorithm\n",
    "        u, v = als.fit(eps_g=grad_sensibility)\n",
    "        # average results\n",
    "        stats = average_stats(stats, als.stats, i + 1)\n",
    "    end = time.time()\n",
    "    # additional context info non depending from experiment results\n",
    "    stats['number_of_ratings'] = trainX.getnnz(\n",
    "    ) if sparse else np.count_nonzero(trainX)\n",
    "    stats['dataset_path'] = data.path\n",
    "    stats['grad_sensibility'] = grad_sensibility\n",
    "    stats['theta_diff_sensibility'] = 1e-10\n",
    "    stats['num_experiments'] = num_experiments\n",
    "    stats['warmup_cycles'] = warmup\n",
    "    stats['experiments_total_runtime'] = end - start\n",
    "#     stats['date'] = date\n",
    "    stats['train_mse'] = als.function_eval() / stats['number_of_ratings']\n",
    "    print(\"Train Mean Squared error is:\", stats['train_mse'])\n",
    "#     print(\"Saving results..\")\n",
    "#     with open(f'data/als_{\"sparse\" if sparse else \"full\"}_{date}.json',\n",
    "#               'w') as f:\n",
    "#         json.dump(stats, f, indent=4)\n",
    "\n",
    "    return stats, als\n",
    "# als = ALS(u=theta[:d.n_users], v=theta[d.n_users:], dataset=X)\n",
    "# us, vs = als.fit()\n",
    "stats, als = run_experiment(d, num_experiments=5, warmup=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0705258430591338"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.function_eval()/np.count_nonzero(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scipy] MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "[Our] MSE: 1.0705258430591338--Norm:515.3069445299816-- n_iter: 195.0\n",
      "Norm differ 15951.42220727992\n",
      "Min/Max/Mean 0.006094939319050761 188.67379019431777 1.8662580137587743 SCipy 0.019146754996978745 60.05977866464008 0.7123748067922835\n",
      "[Scipy] MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "[Our] MSE: 1.0705258430591338--Norm:515.3069445299816-- n_iter: 195.0\n",
      "Norm differ 15951.42220727992\n",
      "Min/Max/Mean 0.006094939319050761 188.67379019431777 1.8662580137587743 SCipy 0.019146754996978745 60.05977866464008 0.7123748067922835\n",
      "[Scipy] MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "[Our] MSE: 1.0705258430591338--Norm:515.3069445299816-- n_iter: 195.0\n",
      "Norm differ 15951.42220727992\n",
      "Min/Max/Mean 0.006094939319050761 188.67379019431777 1.8662580137587743 SCipy 0.019146754996978745 60.05977866464008 0.7123748067922835\n",
      "[Scipy] MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "[Our] MSE: 1.0705258430591338--Norm:515.3069445299816-- n_iter: 195.0\n",
      "Norm differ 15951.42220727992\n",
      "Min/Max/Mean 0.006094939319050761 188.67379019431777 1.8662580137587743 SCipy 0.019146754996978745 60.05977866464008 0.7123748067922835\n",
      "[Scipy] MSE: 1.0705258431350944--Norm:164.08855618262876-- n_iter: 851\n",
      "[Our] MSE: 1.0705258430591338--Norm:515.3069445299816-- n_iter: 195.0\n",
      "Norm differ 15951.42220727992\n",
      "Min/Max/Mean 0.006094939319050761 188.67379019431777 1.8662580137587743 SCipy 0.019146754996978745 60.05977866464008 0.7123748067922835\n"
     ]
    }
   ],
   "source": [
    "our_theta = np.vstack([als.u, als.v])\n",
    "for sol in sc_solutions:\n",
    "    print(f'[Scipy] MSE: {f(sol.x, d.n_users)/np.count_nonzero(X)}--Norm:{np.linalg.norm(sol.x)}-- n_iter: {sol.nit}')\n",
    "    print(f'[Our] MSE: {stats[\"mse\"]}--Norm:{np.linalg.norm(our_theta)}-- n_iter: {stats[\"num_iterations\"]}')\n",
    "    print(\"Norm differ\", np.linalg.norm(our_theta-sol.x))\n",
    "    print(\"Min/Max/Mean\", our_theta.min(), our_theta.max(), our_theta.mean(), 'SCipy',sol.x.min(), sol.x.max(), sol.x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0377603054046634"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats['total_convergence_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "usol = sol.x[:d.n_users].reshape(-1,)\n",
    "myu = als.u.reshape(-1,)\n",
    "plt.figure(figsize=(16,10), dpi=100)\n",
    "plt.plot(usol, label='Scipy BFG')\n",
    "plt.fill_between(np.arange(10), usol, np.zeros(10), alpha=0.2)\n",
    "plt.fill_between(np.arange(10), myu, usol, alpha=0.2)\n",
    "plt.plot(myu, label='Our ALS')\n",
    "plt.xticks(fontsize=14, alpha=.7)\n",
    "# plt.xticks([])\n",
    "plt.title(\"$u^*$ vector comparison\", fontsize=18)\n",
    "plt.yticks(fontsize=12, alpha=.7)\n",
    "# plt.xlim(0,9)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm differ 41.59103196246087\n"
     ]
    }
   ],
   "source": [
    "# normalized diff\n",
    "ot = our_theta/np.linalg.norm(our_theta)\n",
    "xyz = sol.x/np.linalg.norm(sol.x)\n",
    "print(\"Norm differ\", np.linalg.norm(ot-xyz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
